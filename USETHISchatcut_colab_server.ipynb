{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatCut Colab Server\n",
    "\n",
    "**Run cells 1-7 in order, then copy the URL into Premiere Pro.**\n",
    "\n",
    "| Cell | What it does |\n",
    "|------|-------------|\n",
    "| 1 | Install dependencies |\n",
    "| 2 | Imports & config |\n",
    "| 3 | Tracking functions |\n",
    "| 4 | Effect parser |\n",
    "| 5 | Keyframe planner |\n",
    "| 6 | Renderers |\n",
    "| 7 | **START SERVER** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è Using existing GOOGLE_API_KEY env var (if set)\n"
     ]
    }
   ],
   "source": [
    "#@title 0. API Keys (Optional)\n",
    "# Set your Gemini API key for this runtime.\\n\n",
    "import os\n",
    "GOOGLE_API_KEY = \"\"  # @param {type:\"string\"}\n",
    "if GOOGLE_API_KEY:\n",
    "    os.environ['GOOGLE_API_KEY'] = YOUR_GEMINI_API_KEY\n",
    "    print('‚úÖ GOOGLE_API_KEY set for this runtime')\n",
    "else:\n",
    "    print('‚ÑπÔ∏è Using existing GOOGLE_API_KEY env var (if set)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1. Install Dependencies\n",
    "!pip install -q ultralytics opencv-python moviepy numpy pandas tqdm scipy fastapi uvicorn python-multipart lapx pyngrok nest-asyncio transformers pillow google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Device: CUDA\n",
      "‚úÖ Exports: /content/exports\n",
      "üß™ TEST_MODE: False (full quality)\n",
      "üì• Loading CLIP model (ViT-L/14)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n",
      "\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CLIP loaded (semantic object matching enabled)\n",
      "‚úÖ Gemini client initialized (gemini-2.5-flash)\n"
     ]
    }
   ],
   "source": [
    "#@title 2. Imports & Config\n",
    "from __future__ import annotations\n",
    "import json, math, os, re, tempfile, traceback\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import savgol_filter\n",
    "from moviepy.editor import VideoFileClip\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "\n",
    "# SigLIP for semantic object matching\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "#============================================================\n",
    "# CONFIGURATION\n",
    "#============================================================\n",
    "TEST_MODE = False  # Set True for fast 480p rendering, False for production quality\n",
    "#============================================================\n",
    "\n",
    "BASE_DIR = Path('/content')\n",
    "EXPORT_DIR = BASE_DIR / 'exports'\n",
    "EXPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DEFAULT_MODELS = {'det': 'yolo11n.pt', 'seg': 'yolo11n-seg.pt'}\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f\"‚úÖ Device: {DEVICE.upper()}\")\n",
    "print(f\"‚úÖ Exports: {EXPORT_DIR}\")\n",
    "print(f\"üß™ TEST_MODE: {TEST_MODE}\" + (\" (480p, ultrafast)\" if TEST_MODE else \" (full quality)\"))\n",
    "if DEVICE == 'cpu':\n",
    "    print('‚ö†Ô∏è No GPU - enable T4 runtime for better performance')\n",
    "\n",
    "# Load SigLIP model for semantic object selection\n",
    "print(\"üì• Loading CLIP model (ViT-L/14)...\")\n",
    "SIGLIP_MODEL = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(DEVICE)\n",
    "SIGLIP_PROCESSOR = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "SIGLIP_MODEL.eval()\n",
    "print(\"‚úÖ CLIP loaded (semantic object matching enabled)\")\n",
    "\n",
    "# Optional: Gemini 2.5 Flash Lite for visual reranking\n",
    "GEMINI_CLIENT = None\n",
    "GEMINI_TYPES = None\n",
    "GEMINI_MODEL_ID = \"gemini-2.5-flash\"\n",
    "\n",
    "# Hardcoded API key (preferred here due to VS Code Colab env limitations).\n",
    "# Replace \"YOUR_GEMINI_API_KEY\" with your actual key. Leave empty to fall back to env var.\n",
    "GEMINI_API_KEY = \"YOUR_GEMINI_API_KEY\"\n",
    "\n",
    "try:\n",
    "    from google import genai\n",
    "    from google.genai import types as genai_types\n",
    "    # Allow user to paste key after the placeholder or set env vars.\n",
    "    raw_key = (GEMINI_API_KEY or \"\").strip()\n",
    "    placeholder = \"YOUR_GEMINI_API_KEY\"\n",
    "    if raw_key.startswith(placeholder):\n",
    "        raw_key = raw_key[len(placeholder):].strip()\n",
    "    if not raw_key:\n",
    "        # Fallback to common env var names from AI Studio docs\n",
    "        raw_key = (os.environ.get(\"GEMINI_API_KEY\") or\n",
    "                   os.environ.get(\"GOOGLE_GENERATIVE_AI_API_KEY\") or\n",
    "                   os.environ.get(\"GOOGLE_API_KEY\"))\n",
    "    if not raw_key:\n",
    "        raise RuntimeError(\"Gemini API key not set. Set GEMINI_API_KEY or GEMINI_API_KEY/GOOGLE_GENERATIVE_AI_API_KEY env vars.\")\n",
    "    GEMINI_CLIENT = genai.Client(api_key=raw_key)\n",
    "    GEMINI_TYPES = genai_types\n",
    "    print(f\"‚úÖ Gemini client initialized ({GEMINI_MODEL_ID})\")\n",
    "except Exception as e:\n",
    "    print(\"‚ÑπÔ∏è Gemini not available (optional):\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Pre-downloading YOLO models...\n",
      "Loading yolo11n.pt...\n",
      "Loading yolo11n-seg.pt...\n",
      "‚úÖ Models cached and ready!\n",
      "‚úÖ Tracking functions loaded\n"
     ]
    }
   ],
   "source": [
    "#@title 3. Tracking Functions\n",
    "MODEL_CACHE = {}\n",
    "CLASS_NAME_CACHE = {}\n",
    "\n",
    "def load_model(use_seg=False):\n",
    "    name = DEFAULT_MODELS['seg'] if use_seg else DEFAULT_MODELS['det']\n",
    "    if name not in MODEL_CACHE:\n",
    "        print(f'Loading {name}...')\n",
    "        MODEL_CACHE[name] = YOLO(name)\n",
    "    return MODEL_CACHE[name]\n",
    "\n",
    "def _build_name_map(model):\n",
    "    key = id(model)\n",
    "    if key not in CLASS_NAME_CACHE:\n",
    "        names = getattr(model, 'names', {}) or {}\n",
    "        CLASS_NAME_CACHE[key] = {int(k): str(v) for k, v in names.items()} if isinstance(names, dict) else {i: str(v) for i, v in enumerate(names)}\n",
    "    return CLASS_NAME_CACHE[key]\n",
    "\n",
    "def _normalize_label(text): return text.strip().lower()\n",
    "\n",
    "def encode_mask(mask):\n",
    "    mask = (mask > 0.5).astype(np.uint8).flatten(order='F')\n",
    "    counts, last, run = [], 0, 0\n",
    "    for v in mask:\n",
    "        if v == last: run += 1\n",
    "        else: counts.append(run); run = 1; last = v\n",
    "    counts.append(run)\n",
    "    if mask.size and mask[0] == 1: counts = [0] + counts\n",
    "    return {'size': [int(mask.shape[0]), 1], 'counts': counts}\n",
    "\n",
    "def decode_mask(rle):\n",
    "    h, w = rle['size']\n",
    "    vals = []\n",
    "    cur = 0\n",
    "    for c in rle['counts']:\n",
    "        vals.extend([cur] * c)\n",
    "        cur = 1 - cur\n",
    "    return np.array(vals, dtype=np.uint8).reshape((h, w), order='F')\n",
    "\n",
    "def detect_and_track(video_path, use_seg=True, frame_stride=1, conf=0.25, iou=0.45, imgsz=960, save_json=False):\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
    "    w, h = int(cap.get(3)), int(cap.get(4))\n",
    "    duration = total / fps if fps else 0\n",
    "    cap.release()\n",
    "\n",
    "    model = load_model(use_seg)\n",
    "    name_map = _build_name_map(model)\n",
    "    \n",
    "    print(f'Tracking {video_path} @ {fps:.1f}fps | {w}x{h}')\n",
    "    stream = model.track(source=str(video_path), imgsz=imgsz, tracker='bytetrack.yaml', stream=True,\n",
    "                         conf=conf, iou=iou, vid_stride=frame_stride, device=DEVICE, verbose=False, persist=True)\n",
    "    \n",
    "    frames, cursor = [], 0\n",
    "    for result in tqdm(stream, desc='Tracking', total=math.ceil(total/frame_stride)):\n",
    "        dets = []\n",
    "        if result.boxes is not None and result.boxes.id is not None:\n",
    "            ids = result.boxes.id.int().cpu().tolist()\n",
    "            xyxy = result.boxes.xyxy.cpu().tolist()\n",
    "            confs = result.boxes.conf.cpu().tolist()\n",
    "            clss = result.boxes.cls.int().cpu().tolist()\n",
    "            masks = result.masks.data.cpu().numpy() if use_seg and result.masks else None\n",
    "            for i, tid in enumerate(ids):\n",
    "                dets.append({'id': int(tid), 'cls': name_map.get(clss[i], str(clss[i])),\n",
    "                            'conf': float(confs[i]), 'bbox_xyxy': [float(v) for v in xyxy[i]],\n",
    "                            'mask_rle': encode_mask(masks[i]) if masks is not None else None})\n",
    "        frames.append({'frame_index': cursor, 't': cursor/fps, 'detections': dets})\n",
    "        cursor += frame_stride\n",
    "    \n",
    "    return {'video_path': str(video_path), 'fps': fps, 'size': [w, h], 'duration': duration, 'frames': frames}\n",
    "\n",
    "# Pre-download models to avoid delay on first request\n",
    "print(\"üì• Pre-downloading YOLO models...\")\n",
    "_ = load_model(use_seg=False)  # Detection model\n",
    "_ = load_model(use_seg=True)   # Segmentation model\n",
    "print(\"‚úÖ Models cached and ready!\")\n",
    "print('‚úÖ Tracking functions loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Effect parser loaded (Gemini only)\n",
      "   ‚Ä¢ Available effects: ['ZoomFollow', 'Spotlight', 'BlurBackground', 'PixelateObject', 'AutoReframe', 'Callout', 'PiPMagnifier', 'PathOverlay']\n"
     ]
    }
   ],
   "source": [
    "#@title 4. Effect Parser (Gemini only)\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Any\n",
    "\n",
    "EFFECT_DEFAULTS = {\n",
    "    'ZoomFollow': {'margin': 0.10}, 'Spotlight': {'strength': 0.7, 'feather': 45},\n",
    "    'BlurBackground': {'ksize': 21}, 'PixelateObject': {'block': 20},\n",
    "    'AutoReframe': {'aspect': '9:16'}, 'Callout': {'label': 'object'},\n",
    "    'PiPMagnifier': {'scale': 1.5, 'radius': 120}, 'PathOverlay': {}\n",
    "}\n",
    "\n",
    "EFFECT_KEYWORDS = {\n",
    "    'ZoomFollow': ['zoom', 'punch in', 'follow'], 'Spotlight': ['spotlight', 'highlight'],\n",
    "    'BlurBackground': ['blur background', 'background blur'], 'PixelateObject': ['pixelate'],\n",
    "    'AutoReframe': ['reframe', 'vertical'], 'Callout': ['callout', 'label'],\n",
    "    'PiPMagnifier': ['pip', 'magnifier'], 'PathOverlay': ['path', 'trajectory']\n",
    "}\n",
    "\n",
    "@dataclass\n",
    "class EffectCommand:\n",
    "    effect: str\n",
    "    object: str\n",
    "    t_in: float\n",
    "    t_out: float\n",
    "    params: Dict[str, Any] = field(default_factory=dict)\n",
    "    hints: List[str] = field(default_factory=list)\n",
    "    ordinal: Optional[int] = None\n",
    "\n",
    "\n",
    "def parse_nl_to_dsl(cmd: str, duration: float):\n",
    "    \"\"\"Parse natural language to EffectCommand list using Gemini only.\"\"\"\n",
    "    if GEMINI_CLIENT is None or GEMINI_TYPES is None:\n",
    "        raise ValueError(\"Gemini client not initialized; set GEMINI_API_KEY.\")\n",
    "\n",
    "    schema_hint = \"\"\"\n",
    "You are ChatCut's command parser. Return ONLY JSON.\n",
    "Format: {\\\"effects\\\": [ { \\\"effect\\\": one of [ZoomFollow,Spotlight,BlurBackground,PixelateObject,AutoReframe,Callout,PathOverlay],\n",
    "\\\"object\\\": string, \\\"spatial_hint\\\": leftmost/rightmost/center/null, \\\"ordinal\\\": integer or null,\n",
    "\\\"t_in\\\": float, \\\"t_out\\\": float, \\\"label\\\": string or null } ] }.\n",
    "Ensure 0 <= t_in < t_out <= duration. No extra text or markdown.\n",
    "\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Video duration: {duration:.3f} seconds.\n",
    "User command: '{cmd}'.\n",
    "Return ONLY the JSON object, nothing else.\n",
    "\"\"\"\n",
    "\n",
    "    content = GEMINI_TYPES.Content(parts=[\n",
    "        GEMINI_TYPES.Part(text=schema_hint + \"\\n\\n\" + prompt)\n",
    "    ])\n",
    "\n",
    "    import json as _json\n",
    "    import re as _re\n",
    "\n",
    "    # Prefer JSON mime type when available\n",
    "    try:\n",
    "        config = GEMINI_TYPES.GenerateContentConfig(response_mime_type=\"application/json\")\n",
    "    except Exception:\n",
    "        config = None\n",
    "\n",
    "    try:\n",
    "        if config:\n",
    "            resp = GEMINI_CLIENT.models.generate_content(model=GEMINI_MODEL_ID, contents=content, config=config)\n",
    "        else:\n",
    "            resp = GEMINI_CLIENT.models.generate_content(model=GEMINI_MODEL_ID, contents=content)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Gemini request failed: {e}\")\n",
    "\n",
    "    def _extract_text(resp_obj):\n",
    "        raw_text = getattr(resp_obj, 'text', '') or ''\n",
    "        raw_text = raw_text.strip()\n",
    "        if raw_text:\n",
    "            return raw_text\n",
    "        try:\n",
    "            parts = []\n",
    "            for cand in getattr(resp_obj, 'candidates', []) or []:\n",
    "                c_content = getattr(cand, 'content', None)\n",
    "                if c_content is None:\n",
    "                    continue\n",
    "                for part in getattr(c_content, 'parts', []) or []:\n",
    "                    t = getattr(part, 'text', None)\n",
    "                    if t:\n",
    "                        parts.append(t)\n",
    "            return \"\\n\".join(parts).strip()\n",
    "        except Exception:\n",
    "            return ''\n",
    "\n",
    "    raw = _extract_text(resp)\n",
    "    if not raw:\n",
    "        raise ValueError(\"Gemini returned empty response\")\n",
    "\n",
    "    if raw.startswith('```'):\n",
    "        stripped = raw.strip('`')\n",
    "        if stripped.lower().startswith('json'):\n",
    "            stripped = stripped[4:].lstrip()\n",
    "        raw = stripped\n",
    "\n",
    "    try:\n",
    "        data = _json.loads(raw)\n",
    "    except Exception:\n",
    "        m = _re.search(r\"{.*}\", raw, _re.S)\n",
    "        if not m:\n",
    "            raise ValueError(\"Gemini response not JSON\")\n",
    "        data = _json.loads(m.group(0))\n",
    "\n",
    "    effects = data.get('effects') or []\n",
    "    if not effects:\n",
    "        raise ValueError(\"Gemini returned no effects\")\n",
    "\n",
    "    commands = []\n",
    "    for eff in effects:\n",
    "        effect_name = eff.get('effect')\n",
    "        if effect_name not in EFFECT_DEFAULTS:\n",
    "            continue\n",
    "        obj = eff.get('object') or 'person'\n",
    "        spatial = eff.get('spatial_hint') or None\n",
    "        ordinal = eff.get('ordinal')\n",
    "        if isinstance(ordinal, float):\n",
    "            ordinal = int(ordinal)\n",
    "        if not isinstance(ordinal, int):\n",
    "            ordinal = None\n",
    "        t_in = float(max(0.0, min(duration, eff.get('t_in', 0.0))))\n",
    "        t_out = float(max(t_in + 1e-3, min(duration, eff.get('t_out', duration))))\n",
    "        label = eff.get('label')\n",
    "\n",
    "        hints = []\n",
    "        if spatial in ('leftmost', 'rightmost', 'center'):\n",
    "            hints.append(spatial)\n",
    "\n",
    "        params = dict(EFFECT_DEFAULTS.get(effect_name, {}))\n",
    "        if effect_name == 'Callout' and label:\n",
    "            params['label'] = label\n",
    "\n",
    "        commands.append(EffectCommand(\n",
    "            effect=effect_name,\n",
    "            object=obj,\n",
    "            t_in=t_in,\n",
    "            t_out=t_out,\n",
    "            params=params,\n",
    "            hints=hints,\n",
    "            ordinal=ordinal,\n",
    "        ))\n",
    "\n",
    "    if not commands:\n",
    "        raise ValueError(\"Gemini produced no valid commands\")\n",
    "    return commands\n",
    "\n",
    "print('‚úÖ Effect parser loaded (Gemini only)')\n",
    "print('   ‚Ä¢ Available effects:', list(EFFECT_KEYWORDS.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Keyframe planner loaded (v5 - Enhanced Logging)\n",
      "   ‚Ä¢ Min continuity: 50% (general), 70% (callout)\n",
      "   ‚Ä¢ CLIP model: ViT-L/14\n",
      "   ‚Ä¢ Gemini rerank: enabled\n"
     ]
    }
   ],
   "source": [
    "#@title 5. Keyframe Planner (Fixed SigLIP + Callout + Smoothing)\n",
    "tracks_df_cached = None\n",
    "\n",
    "# Minimum continuity thresholds\n",
    "MIN_CONTINUITY = 0.5          # General: at least 50% of frames\n",
    "MIN_CALLOUT_CONTINUITY = 0.7  # Callout needs higher stability (70%)\n",
    "\n",
    "def tracks_to_df(tracks):\n",
    "    recs = []\n",
    "    for f in tracks['frames']:\n",
    "        for d in f['detections']:\n",
    "            x1,y1,x2,y2 = d['bbox_xyxy']\n",
    "            recs.append({'t': f['t'], 'frame_index': f['frame_index'], 'id': d['id'],\n",
    "                        'cls': _normalize_label(d['cls']), 'conf': d['conf'],\n",
    "                        'x1': x1, 'y1': y1, 'x2': x2, 'y2': y2, 'mask_rle': d.get('mask_rle')})\n",
    "    return pd.DataFrame(recs)\n",
    "\n",
    "def get_tracks_df(tracks):\n",
    "    global tracks_df_cached\n",
    "    if tracks_df_cached is None: tracks_df_cached = tracks_to_df(tracks)\n",
    "    return tracks_df_cached\n",
    "\n",
    "def smooth(vals, wl=25, po=2):\n",
    "    \"\"\"Ultra-smooth motion for stable camera tracking.\"\"\"\n",
    "    vals = np.asarray(vals)\n",
    "    if len(vals) < wl:\n",
    "        out = [vals[0]]\n",
    "        for v in vals[1:]:\n",
    "            out.append(0.05*v + 0.95*out[-1])\n",
    "        return np.array(out)\n",
    "    \n",
    "    smoothed = savgol_filter(vals, wl, po)\n",
    "    wl2 = min(15, len(smoothed))\n",
    "    if wl2 >= 5:\n",
    "        smoothed = savgol_filter(smoothed, wl2, po)\n",
    "    out = [smoothed[0]]\n",
    "    for v in smoothed[1:]:\n",
    "        out.append(0.2*v + 0.8*out[-1])\n",
    "    return np.array(out)\n",
    "\n",
    "def clean_object_for_siglip(obj):\n",
    "    \"\"\"Transform parsed object into clean SigLIP query.\"\"\"\n",
    "    stop_phrases = [\n",
    "        'and put', 'and add', 'and apply', 'put a', 'add a', 'put the',\n",
    "        'callout', 'zoom', 'spotlight', 'blur', 'pixelate', 'label',\n",
    "        'with the text', 'saying', 'with text', 'the text', 'text saying',\n",
    "        'in on', 'on the', 'on a', 'the the', 'and a', 'and the',\n",
    "    ]\n",
    "    \n",
    "    result = obj.lower().strip()\n",
    "    for phrase in stop_phrases:\n",
    "        result = result.replace(phrase, ' ')\n",
    "    \n",
    "    result = ' '.join(result.split())\n",
    "    \n",
    "    if not result or result in ['and', 'the', 'a', 'an', 'on', 'in', 'to', 'with']:\n",
    "        return 'a person'\n",
    "    \n",
    "    if not result.startswith(('a ', 'an ', 'the ')):\n",
    "        if result[0] in 'aeiou':\n",
    "            result = f'an {result}'\n",
    "        else:\n",
    "            result = f'a {result}'\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def build_clip_query(obj_clean, effect=None):\n",
    "    \"\"\"Expand a clean object phrase into a richer CLIP text query.\"\"\"\n",
    "    base = obj_clean.strip()\n",
    "    lower = base.lower()\n",
    "    details = []\n",
    "\n",
    "    if any(k in lower for k in ['newspaper', 'paper', 'magazine']):\n",
    "        details.append('holding a newspaper in front of their body')\n",
    "    if any(k in lower for k in ['man', 'woman', 'guy', 'person']):\n",
    "        details.append('main person in the shot')\n",
    "\n",
    "    if effect == 'ZoomFollow':\n",
    "        details.append('framed from the waist up, clearly visible')\n",
    "    elif effect == 'Callout':\n",
    "        details.append('good candidate for a label, unobstructed')\n",
    "\n",
    "    details.append('not on a TV screen, not a projected image')\n",
    "    details.append('not a tiny background figure, not far away')\n",
    "\n",
    "    full = base\n",
    "    if details:\n",
    "        full = base + ', ' + ', '.join(details)\n",
    "    return full\n",
    "\n",
    "\n",
    "def gemini_rerank_tracks(video_path, win_df, candidate_df, user_description, top_k=3):\n",
    "    \"\"\"Use Gemini 2.5 Flash to rerank top candidate tracks visually.\"\"\"\n",
    "    if GEMINI_CLIENT is None or GEMINI_TYPES is None:\n",
    "        return None\n",
    "    if candidate_df.empty:\n",
    "        return None\n",
    "\n",
    "    print(f\"\\n   {'‚îÄ'*50}\")\n",
    "    print(f\"   ü§ñ GEMINI VISUAL RERANK\")\n",
    "    print(f\"   {'‚îÄ'*50}\")\n",
    "    print(f\"      Query: '{user_description}'\")\n",
    "    print(f\"      Candidates: {min(top_k, len(candidate_df))}\")\n",
    "\n",
    "    try:\n",
    "        import cv2\n",
    "        import numpy as np\n",
    "        import re as _re\n",
    "\n",
    "        if 'final_score' in candidate_df.columns:\n",
    "            cand = candidate_df.nlargest(min(top_k, len(candidate_df)), 'final_score')\n",
    "        else:\n",
    "            cand = candidate_df.copy()\n",
    "\n",
    "        crops = []\n",
    "        tids = []\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        for _, row in cand.iterrows():\n",
    "            tid = int(row['id'])\n",
    "            t_data = win_df[win_df['id'] == tid]\n",
    "            if t_data.empty:\n",
    "                continue\n",
    "            frame_idx = int(t_data['frame_index'].median())\n",
    "            x1 = int(t_data['x1'].median())\n",
    "            y1 = int(t_data['y1'].median())\n",
    "            x2 = int(t_data['x2'].median())\n",
    "            y2 = int(t_data['y2'].median())\n",
    "\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                continue\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            crop = frame_rgb[max(0, y1):max(0, min(frame_rgb.shape[0], y2)),\n",
    "                             max(0, x1):max(0, min(frame_rgb.shape[1], x2))]\n",
    "            if crop.size == 0:\n",
    "                continue\n",
    "            ok, buf = cv2.imencode('.jpg', cv2.cvtColor(crop, cv2.COLOR_RGB2BGR))\n",
    "            if not ok:\n",
    "                continue\n",
    "            crops.append(buf.tobytes())\n",
    "            tids.append(tid)\n",
    "            print(f\"      ‚Ä¢ Track {tid}: frame {frame_idx}, bbox [{x1},{y1},{x2},{y2}]\")\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        if len(crops) < 2:\n",
    "            print(f\"      ‚ö†Ô∏è  Not enough crops for rerank ({len(crops)})\")\n",
    "            return None\n",
    "\n",
    "        prompt = (\n",
    "            f\"You will see {len(crops)} images in order. \"\n",
    "            \"Image 1 is the first, image 2 the second, etc. \"\n",
    "            f\"Which image best matches this description: '{user_description}'? \"\n",
    "            \"Reply with just the number (1, 2, or 3).\"\n",
    "        )\n",
    "\n",
    "        parts = [GEMINI_TYPES.Part(text=prompt)]\n",
    "        for data in crops:\n",
    "            parts.append(GEMINI_TYPES.Part(inline_data=GEMINI_TYPES.Blob(data=data, mime_type='image/jpeg')))\n",
    "\n",
    "        content = GEMINI_TYPES.Content(parts=parts)\n",
    "        \n",
    "        import time as _time\n",
    "        start_time = _time.time()\n",
    "        resp = GEMINI_CLIENT.models.generate_content(model=GEMINI_MODEL_ID, contents=content)\n",
    "        api_time = _time.time() - start_time\n",
    "        \n",
    "        text = getattr(resp, 'text', '') or ''\n",
    "        print(f\"      ‚ö° API time: {api_time:.2f}s\")\n",
    "        print(f\"      üì® Response: '{text.strip()}'\")\n",
    "        \n",
    "        m = _re.search(r'[1-9]', text)\n",
    "        if not m:\n",
    "            print(f\"      ‚ö†Ô∏è  Could not parse selection from response\")\n",
    "            return None\n",
    "        idx = int(m.group(0)) - 1\n",
    "        if idx < 0 or idx >= len(tids):\n",
    "            print(f\"      ‚ö†Ô∏è  Invalid index {idx+1} (have {len(tids)} candidates)\")\n",
    "            return None\n",
    "        \n",
    "        selected_tid = int(tids[idx])\n",
    "        print(f\"      ‚úÖ Gemini selected: Track {selected_tid} (image {idx+1})\")\n",
    "        print(f\"   {'‚îÄ'*50}\")\n",
    "        return selected_tid\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      ‚ùå Gemini rerank failed: {e}\")\n",
    "        print(f\"   {'‚îÄ'*50}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def choose_track_by_layout(df, t_in, t_out, frame_size, hints=None, ordinal=None, fps=30.0, min_continuity=MIN_CONTINUITY):\n",
    "    \"\"\"Deterministic selection based on left/right/center and ordinal.\"\"\"\n",
    "    hints = hints or []\n",
    "    W, H = frame_size\n",
    "\n",
    "    print(f\"\\n   {'‚îÄ'*50}\")\n",
    "    print(f\"   üìê LAYOUT-BASED SELECTION\")\n",
    "    print(f\"   {'‚îÄ'*50}\")\n",
    "    print(f\"      Hints: {hints}\")\n",
    "    print(f\"      Ordinal: {ordinal}\")\n",
    "\n",
    "    win = df[(df['t'] >= t_in) & (df['t'] <= t_out)]\n",
    "    if win.empty:\n",
    "        raise ValueError(f'No detections found in time {t_in:.1f}s-{t_out:.1f}s')\n",
    "\n",
    "    win = win[win['cls'] == 'person']\n",
    "    if win.empty:\n",
    "        raise ValueError('No person detections found in the selected time window')\n",
    "\n",
    "    track_stats = []\n",
    "    time_window = t_out - t_in\n",
    "    expected_frames = max(time_window * fps, 1)\n",
    "\n",
    "    for tid in win['id'].unique():\n",
    "        t_data = win[win['id'] == tid]\n",
    "        frame_count = len(t_data)\n",
    "        continuity = min(frame_count / expected_frames, 1.0)\n",
    "        x1 = float(t_data['x1'].median())\n",
    "        y1 = float(t_data['y1'].median())\n",
    "        x2 = float(t_data['x2'].median())\n",
    "        y2 = float(t_data['y2'].median())\n",
    "        track_stats.append({\n",
    "            'id': int(tid),\n",
    "            'continuity': continuity,\n",
    "            'avg_x': (x1 + x2) / 2,\n",
    "            'avg_y': (y1 + y2) / 2,\n",
    "            'avg_width': x2 - x1,\n",
    "            'avg_height': y2 - y1,\n",
    "        })\n",
    "\n",
    "    stats_df = pd.DataFrame(track_stats)\n",
    "    if stats_df.empty:\n",
    "        raise ValueError('No valid person tracks for layout selection')\n",
    "\n",
    "    print(f\"      Found {len(stats_df)} person tracks\")\n",
    "\n",
    "    valid_df = stats_df[stats_df['continuity'] >= min_continuity]\n",
    "    if valid_df.empty:\n",
    "        print(f\"      ‚ö†Ô∏è  No tracks with ‚â•{min_continuity*100:.0f}% continuity, using all\")\n",
    "        valid_df = stats_df\n",
    "    else:\n",
    "        print(f\"      Filtered to {len(valid_df)} tracks (‚â•{min_continuity*100:.0f}% continuity)\")\n",
    "\n",
    "    sort_left = True\n",
    "    if 'rightmost' in hints:\n",
    "        sort_left = False\n",
    "\n",
    "    if ordinal is not None:\n",
    "        ordered = valid_df.sort_values('avg_x', ascending=sort_left).reset_index(drop=True)\n",
    "        idx = max(0, min(int(ordinal), len(ordered) - 1))\n",
    "        best = ordered.iloc[idx]\n",
    "        print(f\"      ‚úÖ Ordinal selection: #{ordinal} from {'left' if sort_left else 'right'}\")\n",
    "        print(f\"      ‚Üí Track {int(best['id'])} at x={best['avg_x']:.0f}\")\n",
    "        print(f\"   {'‚îÄ'*50}\")\n",
    "        return int(best['id'])\n",
    "\n",
    "    if 'leftmost' in hints or 'rightmost' in hints:\n",
    "        ordered = valid_df.sort_values('avg_x', ascending=sort_left)\n",
    "        best = ordered.iloc[0]\n",
    "        print(f\"      ‚úÖ Spatial selection: {'leftmost' if sort_left else 'rightmost'}\")\n",
    "        print(f\"      ‚Üí Track {int(best['id'])} at x={best['avg_x']:.0f}\")\n",
    "        print(f\"   {'‚îÄ'*50}\")\n",
    "        return int(best['id'])\n",
    "\n",
    "    if 'center' in hints:\n",
    "        cx, cy = W / 2, H / 2\n",
    "        valid_df = valid_df.copy()\n",
    "        valid_df['dist_to_center'] = np.sqrt((valid_df['avg_x'] - cx)**2 + (valid_df['avg_y'] - cy)**2)\n",
    "        best = valid_df.nsmallest(1, 'dist_to_center').iloc[0]\n",
    "        print(f\"      ‚úÖ Center selection: closest to ({cx:.0f}, {cy:.0f})\")\n",
    "        print(f\"      ‚Üí Track {int(best['id'])} (dist={best['dist_to_center']:.0f}px)\")\n",
    "        print(f\"   {'‚îÄ'*50}\")\n",
    "        return int(best['id'])\n",
    "\n",
    "    valid_df = valid_df.copy()\n",
    "    valid_df['size'] = valid_df['avg_width'] * valid_df['avg_height']\n",
    "    cx, cy = W / 2, H / 2\n",
    "    valid_df['dist_to_center'] = np.sqrt((valid_df['avg_x'] - cx)**2 + (valid_df['avg_y'] - cy)**2)\n",
    "    valid_df['score'] = valid_df['size'] / (1 + valid_df['dist_to_center'])\n",
    "    best = valid_df.nlargest(1, 'score').iloc[0]\n",
    "    print(f\"      ‚úÖ Fallback: largest central person\")\n",
    "    print(f\"      ‚Üí Track {int(best['id'])} (size={best['size']:.0f}px¬≤)\")\n",
    "    print(f\"   {'‚îÄ'*50}\")\n",
    "    return int(best['id'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def choose_track_gemini_primary(df, user_description, t_in, t_out, video_path, frame_size, fps=30.0, max_tracks=5):\n",
    "    \"\"\"Use Gemini to pick the best matching person track.\n",
    "    Returns track id or None on failure.\n",
    "    \"\"\"\n",
    "    if GEMINI_CLIENT is None or GEMINI_TYPES is None:\n",
    "        return None\n",
    "\n",
    "    # Time window and person filter\n",
    "    win = df[(df['t'] >= t_in) & (df['t'] <= t_out)]\n",
    "    win = win[win['cls'] == 'person']\n",
    "    if win.empty:\n",
    "        return None\n",
    "\n",
    "    W, H = frame_size\n",
    "    time_window = t_out - t_in\n",
    "    expected_frames = max(time_window * fps, 1)\n",
    "\n",
    "    # Score tracks by size * continuity to pick top candidates\n",
    "    stats = []\n",
    "    for tid in win['id'].unique():\n",
    "        t_data = win[win['id'] == tid]\n",
    "        frame_count = len(t_data)\n",
    "        continuity = min(frame_count / expected_frames, 1.0)\n",
    "        x1 = float(t_data['x1'].median())\n",
    "        y1 = float(t_data['y1'].median())\n",
    "        x2 = float(t_data['x2'].median())\n",
    "        y2 = float(t_data['y2'].median())\n",
    "        size = max(1.0, (x2 - x1) * (y2 - y1))\n",
    "        stats.append({'id': int(tid), 'continuity': continuity, 'size': size,\n",
    "                      'x1': x1, 'y1': y1, 'x2': x2, 'y2': y2,\n",
    "                      'frame_index': int(t_data['frame_index'].median())})\n",
    "\n",
    "    if not stats:\n",
    "        return None\n",
    "\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "\n",
    "    # Pick top candidates\n",
    "    stats = sorted(stats, key=lambda s: s['size'] * s['continuity'], reverse=True)[:max_tracks]\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    parts = []\n",
    "    tids = []\n",
    "    for idx_s, s in enumerate(stats, start=1):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, s['frame_index'])\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        x1,y1,x2,y2 = map(int, [s['x1'], s['y1'], s['x2'], s['y2']])\n",
    "        crop = frame_rgb[max(0,y1):min(frame_rgb.shape[0],y2), max(0,x1):min(frame_rgb.shape[1],x2)]\n",
    "        if crop.size == 0:\n",
    "            continue\n",
    "        ok, buf = cv2.imencode('.jpg', cv2.cvtColor(crop, cv2.COLOR_RGB2BGR))\n",
    "        if not ok:\n",
    "            continue\n",
    "        tids.append(s['id'])\n",
    "        parts.append(GEMINI_TYPES.Part(inline_data=GEMINI_TYPES.Blob(data=buf.tobytes(), mime_type='image/jpeg')))\n",
    "    cap.release()\n",
    "\n",
    "    if len(parts) < 1:\n",
    "        return None\n",
    "\n",
    "    prompt = (\n",
    "        f\"You will see {len(parts)} images. Image 1 is first, image 2 is second, etc. \"\n",
    "        f\"Which image best matches this description: '{user_description}'? \"\n",
    "        \"Reply with just the number (1, 2, ...).\"\n",
    "    )\n",
    "\n",
    "    content = GEMINI_TYPES.Content(parts=[GEMINI_TYPES.Part(text=prompt)] + parts)\n",
    "\n",
    "    try:\n",
    "        resp = GEMINI_CLIENT.models.generate_content(model=GEMINI_MODEL_ID, contents=content)\n",
    "        txt = getattr(resp, 'text', '') or ''\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    import re as _re\n",
    "    m = _re.search(r'[1-9]', txt)\n",
    "    if not m:\n",
    "        return None\n",
    "    idx_choice = int(m.group(0)) - 1\n",
    "    if idx_choice < 0 or idx_choice >= len(tids):\n",
    "        return None\n",
    "    return tids[idx_choice]\n",
    "\n",
    "def choose_track_id_siglip(df, user_description, t_in, t_out, video_path, frame_size, \n",
    "                           hints=None, fps=30.0, min_continuity=MIN_CONTINUITY):\n",
    "    \"\"\"Use SigLIP/CLIP to semantically match detections to user description.\"\"\"\n",
    "    hints = hints or []\n",
    "    W, H = frame_size\n",
    "    \n",
    "    print(f\"\\n   {'‚îÄ'*50}\")\n",
    "    print(f\"   üîç CLIP SEMANTIC MATCHING\")\n",
    "    print(f\"   {'‚îÄ'*50}\")\n",
    "    print(f\"      Query: '{user_description}'\")\n",
    "    print(f\"      Time window: {t_in:.2f}s ‚Üí {t_out:.2f}s\")\n",
    "    \n",
    "    win = df[(df['t'] >= t_in) & (df['t'] <= t_out)]\n",
    "    if win.empty:\n",
    "        raise ValueError(f'No detections found in time {t_in:.1f}s-{t_out:.1f}s')\n",
    "    \n",
    "    win = win[win['cls'] == 'person']\n",
    "    if win.empty:\n",
    "        raise ValueError('No person detections found in the selected time window')\n",
    "    \n",
    "    track_ids = win['id'].unique()\n",
    "    print(f\"      Found {len(track_ids)} person tracks to score\")\n",
    "    \n",
    "    track_stats = []\n",
    "    time_window = t_out - t_in\n",
    "    expected_frames = max(time_window * fps, 1)\n",
    "    \n",
    "    for tid in track_ids:\n",
    "        t_data = win[win['id'] == tid]\n",
    "        frame_count = len(t_data)\n",
    "        continuity = min(frame_count / expected_frames, 1.0)\n",
    "        \n",
    "        x1 = int(t_data['x1'].median())\n",
    "        y1 = int(t_data['y1'].median())\n",
    "        x2 = int(t_data['x2'].median())\n",
    "        y2 = int(t_data['y2'].median())\n",
    "\n",
    "        frame_indices = sorted(t_data['frame_index'].unique())\n",
    "        if len(frame_indices) >= 3:\n",
    "            sample_idxs = [frame_indices[0], frame_indices[len(frame_indices)//2], frame_indices[-1]]\n",
    "        else:\n",
    "            sample_idxs = frame_indices\n",
    "\n",
    "        scores = []\n",
    "        for frame_idx in sample_idxs:\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, int(frame_idx))\n",
    "            ret, frame = cap.read()\n",
    "            cap.release()\n",
    "            if not ret:\n",
    "                continue\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            crop = frame_rgb[max(0,y1):min(frame_rgb.shape[0],y2),\n",
    "                            max(0,x1):min(frame_rgb.shape[1],x2)]\n",
    "            if crop.size == 0:\n",
    "                continue\n",
    "\n",
    "            crop_pil = Image.fromarray(crop)\n",
    "            inputs = SIGLIP_PROCESSOR(\n",
    "                images=crop_pil,\n",
    "                text=[user_description],\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"max_length\"\n",
    "            ).to(DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = SIGLIP_MODEL(**inputs)\n",
    "\n",
    "            logit = float(outputs.logits_per_image.item())\n",
    "            score = 1.0 / (1.0 + math.exp(-logit / 4.0))\n",
    "            scores.append(score)\n",
    "\n",
    "        if not scores:\n",
    "            continue\n",
    "\n",
    "        siglip_score = float(sum(scores) / len(scores))\n",
    "        cls = t_data['cls'].iloc[0]\n",
    "        avg_conf = t_data['conf'].mean()\n",
    "\n",
    "        stats = {\n",
    "            'id': tid,\n",
    "            'cls': cls,\n",
    "            'siglip_score': siglip_score,\n",
    "            'continuity': continuity,\n",
    "            'avg_conf': avg_conf,\n",
    "            'count': frame_count,\n",
    "            'avg_x': (x1 + x2) / 2,\n",
    "            'avg_y': (y1 + y2) / 2,\n",
    "            'avg_width': x2 - x1,\n",
    "            'avg_height': y2 - y1,\n",
    "        }\n",
    "        track_stats.append(stats)\n",
    "    \n",
    "    if not track_stats:\n",
    "        raise ValueError(\"No valid detections found\")\n",
    "    \n",
    "    stats_df = pd.DataFrame(track_stats)\n",
    "    \n",
    "    valid_df = stats_df[stats_df['continuity'] >= min_continuity]\n",
    "    if valid_df.empty:\n",
    "        print(f\"      ‚ö†Ô∏è  No tracks with ‚â•{min_continuity*100:.0f}% continuity, using top 3\")\n",
    "        valid_df = stats_df.nlargest(3, 'continuity')\n",
    "    else:\n",
    "        print(f\"      Filtered to {len(valid_df)} tracks (‚â•{min_continuity*100:.0f}% continuity)\")\n",
    "    \n",
    "    if 'leftmost' in hints:\n",
    "        valid_df = valid_df.nsmallest(min(3, len(valid_df)), 'avg_x')\n",
    "        print(f\"      Applied 'leftmost' filter: {len(valid_df)} candidates\")\n",
    "    \n",
    "    if 'rightmost' in hints:\n",
    "        valid_df = valid_df.nlargest(min(3, len(valid_df)), 'avg_x')\n",
    "        print(f\"      Applied 'rightmost' filter: {len(valid_df)} candidates\")\n",
    "    \n",
    "    if 'center' in hints:\n",
    "        cx, cy = W / 2, H / 2\n",
    "        valid_df = valid_df.copy()\n",
    "        valid_df['dist_to_center'] = np.sqrt((valid_df['avg_x'] - cx)**2 + (valid_df['avg_y'] - cy)**2)\n",
    "        valid_df = valid_df.nsmallest(min(3, len(valid_df)), 'dist_to_center')\n",
    "        print(f\"      Applied 'center' filter: {len(valid_df)} candidates\")\n",
    "    \n",
    "    valid_df = valid_df.copy()\n",
    "    cx, cy = W / 2, H / 2\n",
    "    valid_df['dist_to_center'] = np.sqrt((valid_df['avg_x'] - cx)**2 + (valid_df['avg_y'] - cy)**2)\n",
    "    valid_df['dist_norm'] = valid_df['dist_to_center'] / np.sqrt(cx**2 + cy**2)\n",
    "    valid_df['center_norm'] = 1 / (1 + valid_df['dist_norm'])\n",
    "    valid_df['size_norm'] = np.clip((valid_df['avg_width'] * valid_df['avg_height']) / (W * H) * 5.0, 0.5, 2.0)\n",
    "    valid_df['final_score'] = (valid_df['siglip_score'] ** 2) * np.sqrt(valid_df['continuity']) * valid_df['size_norm'] * valid_df['center_norm']\n",
    "\n",
    "    print(f\"\\n      üìä CLIP SCORES (top {min(5, len(valid_df))}):\")\n",
    "    for _, row in valid_df.nlargest(min(5, len(valid_df)), 'final_score').iterrows():\n",
    "        print(f\"         Track {int(row['id']):3d}: CLIP={row['siglip_score']:.3f}, cont={row['continuity']:.2f}, final={row['final_score']:.4f}\")\n",
    "\n",
    "    max_siglip = float(valid_df['siglip_score'].max())\n",
    "\n",
    "    if max_siglip < 0.25:\n",
    "        best = valid_df.loc[valid_df['dist_to_center'].idxmin()]\n",
    "        print(f\"\\n      ‚ö†Ô∏è  Low CLIP confidence ({max_siglip:.3f} < 0.25)\")\n",
    "        print(f\"      ‚Üí Fallback to most central: Track {int(best['id'])}\")\n",
    "    else:\n",
    "        best = valid_df.loc[valid_df['final_score'].idxmax()]\n",
    "        print(f\"\\n      ‚úÖ CLIP selection: Track {int(best['id'])} (score={best['final_score']:.4f})\")\n",
    "\n",
    "        if GEMINI_CLIENT is not None and max_siglip < 0.6 and len(valid_df) > 1:\n",
    "            print(f\"      üîÑ CLIP confidence middling ({max_siglip:.3f} < 0.6), trying Gemini rerank...\")\n",
    "            gem_tid = gemini_rerank_tracks(video_path, win, valid_df, user_description)\n",
    "            if gem_tid is not None and gem_tid in valid_df['id'].values:\n",
    "                best = valid_df[valid_df['id'] == gem_tid].iloc[0]\n",
    "                print(f\"      ü§ñ Gemini override: Track {int(best['id'])}\")\n",
    "\n",
    "    print(f\"   {'‚îÄ'*50}\")\n",
    "    return int(best['id'])\n",
    "\n",
    "\n",
    "def choose_track_for_callout(df, user_description, t_in, t_out, video_path, frame_size, \n",
    "                              hints=None, fps=30.0):\n",
    "    \"\"\"Specialized track selection for Callout effect (needs high continuity).\"\"\"\n",
    "    hints = hints or []\n",
    "    W, H = frame_size\n",
    "    \n",
    "    print(f\"\\n   {'‚îÄ'*50}\")\n",
    "    print(f\"   üè∑Ô∏è  CALLOUT TRACK SELECTION\")\n",
    "    print(f\"   {'‚îÄ'*50}\")\n",
    "    print(f\"      Query: '{user_description}'\")\n",
    "    print(f\"      Required continuity: ‚â•{MIN_CALLOUT_CONTINUITY*100:.0f}%\")\n",
    "    \n",
    "    win = df[(df['t'] >= t_in) & (df['t'] <= t_out)]\n",
    "    if win.empty:\n",
    "        raise ValueError(f'No detections found in time {t_in:.1f}s-{t_out:.1f}s')\n",
    "    \n",
    "    track_ids = win['id'].unique()\n",
    "    print(f\"      Found {len(track_ids)} tracks\")\n",
    "    \n",
    "    track_stats = []\n",
    "    time_window = t_out - t_in\n",
    "    expected_frames = max(time_window * fps, 1)\n",
    "    \n",
    "    for tid in track_ids:\n",
    "        t_data = win[win['id'] == tid]\n",
    "        frame_count = len(t_data)\n",
    "        continuity = min(frame_count / expected_frames, 1.0)\n",
    "        \n",
    "        x1 = int(t_data['x1'].median())\n",
    "        y1 = int(t_data['y1'].median())\n",
    "        x2 = int(t_data['x2'].median())\n",
    "        y2 = int(t_data['y2'].median())\n",
    "        frame_idx = int(t_data['frame_index'].median())\n",
    "        \n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cap.read()\n",
    "        cap.release()\n",
    "        if not ret:\n",
    "            continue\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        crop = frame_rgb[max(0,y1):min(frame_rgb.shape[0],y2),\n",
    "                        max(0,x1):min(frame_rgb.shape[1],x2)]\n",
    "        \n",
    "        if crop.size == 0:\n",
    "            continue\n",
    "        \n",
    "        crop_pil = Image.fromarray(crop)\n",
    "        inputs = SIGLIP_PROCESSOR(\n",
    "            images=crop_pil, text=[user_description],\n",
    "            return_tensors=\"pt\", padding=\"max_length\"\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = SIGLIP_MODEL(**inputs)\n",
    "        \n",
    "        logit = float(outputs.logits_per_image.item())\n",
    "        siglip_score = 1.0 / (1.0 + math.exp(-logit / 4.0))\n",
    "        \n",
    "        stats = {\n",
    "            'id': tid,\n",
    "            'cls': t_data['cls'].iloc[0],\n",
    "            'siglip_score': siglip_score,\n",
    "            'continuity': continuity,\n",
    "            'avg_conf': t_data['conf'].mean(),\n",
    "            'count': frame_count,\n",
    "            'avg_x': (x1 + x2) / 2,\n",
    "            'avg_y': (y1 + y2) / 2,\n",
    "            'avg_height': y2 - y1,\n",
    "        }\n",
    "        track_stats.append(stats)\n",
    "    \n",
    "    if not track_stats:\n",
    "        raise ValueError(\"No valid detections\")\n",
    "    \n",
    "    stats_df = pd.DataFrame(track_stats)\n",
    "    \n",
    "    valid_df = stats_df[stats_df['continuity'] >= MIN_CALLOUT_CONTINUITY]\n",
    "    if valid_df.empty:\n",
    "        print(f\"      ‚ö†Ô∏è  No tracks with ‚â•{MIN_CALLOUT_CONTINUITY*100:.0f}% continuity\")\n",
    "        valid_df = stats_df.nlargest(1, 'continuity')\n",
    "    else:\n",
    "        print(f\"      {len(valid_df)} high-continuity tracks\")\n",
    "    \n",
    "    if 'center' in hints:\n",
    "        cx, cy = W / 2, H / 2\n",
    "        valid_df = valid_df.copy()\n",
    "        valid_df['dist_to_center'] = np.sqrt((valid_df['avg_x'] - cx)**2 + (valid_df['avg_y'] - cy)**2)\n",
    "        valid_df = valid_df.nsmallest(min(3, len(valid_df)), 'dist_to_center')\n",
    "    \n",
    "    valid_df = valid_df.copy()\n",
    "    valid_df['final_score'] = (\n",
    "        (valid_df['siglip_score'] ** 2) *\n",
    "        (valid_df['continuity'] ** 2) *\n",
    "        (valid_df['avg_height'] / 100)\n",
    "    )\n",
    "\n",
    "    print(f\"\\n      üìä CALLOUT SCORES:\")\n",
    "    for _, row in valid_df.nlargest(min(3, len(valid_df)), 'final_score').iterrows():\n",
    "        print(f\"         Track {int(row['id']):3d}: CLIP={row['siglip_score']:.3f}, cont={row['continuity']:.2f}, h={row['avg_height']:.0f}px\")\n",
    "\n",
    "    max_siglip = float(valid_df['siglip_score'].max())\n",
    "\n",
    "    if max_siglip < 0.25:\n",
    "        best = valid_df.loc[valid_df['continuity'].idxmax()]\n",
    "        print(f\"\\n      ‚ö†Ô∏è  Low CLIP confidence, using most stable track\")\n",
    "        print(f\"      ‚Üí Track {int(best['id'])} (continuity={best['continuity']:.2f})\")\n",
    "    else:\n",
    "        best = valid_df.loc[valid_df['final_score'].idxmax()]\n",
    "        print(f\"\\n      ‚úÖ Selected: Track {int(best['id'])} (cont={best['continuity']:.2f})\")\n",
    "\n",
    "        if GEMINI_CLIENT is not None and max_siglip < 0.6 and len(valid_df) > 1:\n",
    "            gem_tid = gemini_rerank_tracks(video_path, win, valid_df, user_description)\n",
    "            if gem_tid is not None and gem_tid in valid_df['id'].values:\n",
    "                best = valid_df[valid_df['id'] == gem_tid].iloc[0]\n",
    "                print(f\"      ü§ñ Gemini override: Track {int(best['id'])}\")\n",
    "\n",
    "    print(f\"   {'‚îÄ'*50}\")\n",
    "    return int(best['id'])\n",
    "\n",
    "\n",
    "def plan_effect(cmd, tracks):\n",
    "    \"\"\"Plan effect using SigLIP semantic matching.\"\"\"\n",
    "    df = get_tracks_df(tracks)\n",
    "    fps = tracks.get('fps', 30.0)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìä KEYFRAME PLANNER: {cmd.effect}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"   Raw object: '{cmd.object}'\")\n",
    "    \n",
    "    obj_clean = clean_object_for_siglip(cmd.object)\n",
    "    clip_query = build_clip_query(obj_clean, cmd.effect)\n",
    "    print(f\"   Cleaned: '{obj_clean}'\")\n",
    "    print(f\"   CLIP query: '{clip_query[:80]}{'...' if len(clip_query) > 80 else ''}'\")\n",
    "\n",
    "    hints = getattr(cmd, 'hints', None)\n",
    "    ordinal = getattr(cmd, 'ordinal', None)\n",
    "    use_layout = (ordinal is not None) or (hints and any(h in ['leftmost', 'rightmost'] for h in hints))\n",
    "\n",
    "    if use_layout:\n",
    "        print(f\"   Selection method: LAYOUT (ordinal={ordinal}, hints={hints})\")\n",
    "        tid = choose_track_by_layout(\n",
    "            df, cmd.t_in, cmd.t_out,\n",
    "            tracks['size'],\n",
    "            hints=hints,\n",
    "            ordinal=ordinal,\n",
    "            fps=fps\n",
    "        )\n",
    "    elif cmd.effect == 'Callout':\n",
    "        print(f\"   Selection method: CALLOUT-SPECIALIZED\")\n",
    "        tid = choose_track_for_callout(\n",
    "            df, clip_query, cmd.t_in, cmd.t_out,\n",
    "            tracks['video_path'], tracks['size'],\n",
    "            hints=hints,\n",
    "            fps=fps\n",
    "        )\n",
    "    else:\n",
    "        print(f\"   Selection method: GEMINI ‚Üí CLIP\")\n",
    "        tid = None\n",
    "        if GEMINI_CLIENT is not None and GEMINI_TYPES is not None:\n",
    "            try:\n",
    "                tid = choose_track_gemini_primary(\n",
    "                    df, clip_query, cmd.t_in, cmd.t_out,\n",
    "                    tracks['video_path'], tracks['size'],\n",
    "                    fps=fps\n",
    "                )\n",
    "                if tid is not None:\n",
    "                    print(f\"   ü§ñ Gemini selected track {tid}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è Gemini primary selection failed: {e}\")\n",
    "        if tid is None:\n",
    "            tid = choose_track_id_siglip(\n",
    "                df, clip_query, cmd.t_in, cmd.t_out,\n",
    "                tracks['video_path'], tracks['size'],\n",
    "                hints=hints,\n",
    "                fps=fps\n",
    "            )\n",
    "    \n",
    "    win = df[(df['id'] == tid) & (df['t'] >= cmd.t_in) & (df['t'] <= cmd.t_out)]\n",
    "    \n",
    "    cx = smooth((win['x1'].values + win['x2'].values) / 2)\n",
    "    cy = smooth((win['y1'].values + win['y2'].values) / 2)\n",
    "    W, H = tracks['size']\n",
    "    widths, heights = win['x2'].values - win['x1'].values, win['y2'].values - win['y1'].values\n",
    "    margin = cmd.params.get('margin', 0.1)\n",
    "    scale = smooth(np.maximum(widths/W, heights/H) * (1 + margin))\n",
    "    \n",
    "    timeline = [{'t': float(r.t), 'frame': int(r.frame_index),\n",
    "                 'center': [float(cx[i]), float(cy[i])], 'scale': float(scale[i]),\n",
    "                 'bbox': [float(r.x1), float(r.y1), float(r.x2), float(r.y2)], 'mask_rle': r.mask_rle}\n",
    "                for i, r in enumerate(win.itertuples())]\n",
    "    \n",
    "    print(f\"\\n   üìç FINAL PLAN:\")\n",
    "    print(f\"      Track ID: {tid}\")\n",
    "    print(f\"      Keyframes: {len(timeline)}\")\n",
    "    print(f\"      Time range: {cmd.t_in:.2f}s ‚Üí {cmd.t_out:.2f}s\")\n",
    "    print(f\"      Params: {cmd.params}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return {'effect': cmd.effect, 'object': cmd.object, 'track_id': int(tid),\n",
    "            't_in': cmd.t_in, 't_out': cmd.t_out, 'timeline': timeline,\n",
    "            'frame_size': tracks['size'], 'fps': fps,\n",
    "            'video_path': tracks['video_path'], 'params': cmd.params}\n",
    "\n",
    "print('‚úÖ Keyframe planner loaded (v5 - Enhanced Logging)')\n",
    "print(f'   ‚Ä¢ Min continuity: {MIN_CONTINUITY*100:.0f}% (general), {MIN_CALLOUT_CONTINUITY*100:.0f}% (callout)')\n",
    "print(f'   ‚Ä¢ CLIP model: ViT-L/14')\n",
    "print(f'   ‚Ä¢ Gemini rerank: {\"enabled\" if GEMINI_CLIENT else \"disabled\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Renderers loaded (v3 - Premiere Pro Compatible)\n",
      "   üß™ TEST_MODE: False ‚Üí full quality + Premiere Pro flags\n",
      "   Effects: ['ZoomFollow', 'Spotlight', 'BlurBackground', 'PixelateObject', 'Callout', 'PathOverlay']\n"
     ]
    }
   ],
   "source": [
    "#@title 6. Renderers (with TEST_MODE support + Premiere Pro Compatibility)\n",
    "import subprocess\n",
    "\n",
    "def get_video_encoding_params(video_path):\n",
    "    \"\"\"Read source video FPS and calculate optimal encoding parameters.\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run([\n",
    "            'ffprobe', '-v', 'quiet',\n",
    "            '-select_streams', 'v:0',\n",
    "            '-show_entries', 'stream=r_frame_rate',\n",
    "            '-of', 'csv=p=0',\n",
    "            str(video_path)\n",
    "        ], capture_output=True, text=True)\n",
    "        r_frame = result.stdout.strip()\n",
    "        if '/' in r_frame:\n",
    "            num, den = map(int, r_frame.split('/'))\n",
    "            fps = num / den if den else 30.0\n",
    "        else:\n",
    "            fps = float(r_frame) if r_frame else 30.0\n",
    "        gop_size = int(round(fps))  # 1 keyframe per second\n",
    "        print(f\"Source video: {fps:.2f}fps, GOP={gop_size}\")\n",
    "        return {'fps': fps, 'gop_size': gop_size, 'keyint_min': gop_size}\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not read video params: {e}, using defaults\")\n",
    "        return {'fps': 30.0, 'gop_size': 30, 'keyint_min': 30}\n",
    "\n",
    "\n",
    "def decode_mask_rle(rle, frame_shape):\n",
    "    if not rle:\n",
    "        return None\n",
    "    mask = decode_mask(rle)\n",
    "    if mask.shape != frame_shape:\n",
    "        mask = cv2.resize(mask, (frame_shape[1], frame_shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def timeline_sampler(timeline):\n",
    "    times = np.array([item['t'] for item in timeline], dtype=np.float32)\n",
    "    centers = np.array([item['center'] for item in timeline], dtype=np.float32)\n",
    "    scales = np.array([item['scale'] for item in timeline], dtype=np.float32)\n",
    "    bboxes = np.array([item['bbox'] for item in timeline], dtype=np.float32)\n",
    "    masks = [item.get('mask_rle') for item in timeline]\n",
    "\n",
    "    def sample(t):\n",
    "        if t <= times[0]:\n",
    "            return {'center': centers[0], 'scale': scales[0], 'bbox': bboxes[0], 'mask': masks[0]}\n",
    "        if t >= times[-1]:\n",
    "            return {'center': centers[-1], 'scale': scales[-1], 'bbox': bboxes[-1], 'mask': masks[-1]}\n",
    "        idx = np.searchsorted(times, t, side='right')\n",
    "        i0 = max(idx - 1, 0)\n",
    "        i1 = min(idx, len(times) - 1)\n",
    "        span = (times[i1] - times[i0]) or 1e-6\n",
    "        alpha = (t - times[i0]) / span\n",
    "        center = centers[i0] * (1 - alpha) + centers[i1] * alpha\n",
    "        scale = scales[i0] * (1 - alpha) + scales[i1] * alpha\n",
    "        bbox = bboxes[i0] * (1 - alpha) + bboxes[i1] * alpha\n",
    "        mask = masks[i0 if alpha <= 0.5 else i1]\n",
    "        return {'center': center, 'scale': scale, 'bbox': bbox, 'mask': mask}\n",
    "\n",
    "    return sample\n",
    "\n",
    "\n",
    "def ensure_mask(state, frame_shape, feather=25):\n",
    "    mask = decode_mask_rle(state.get('mask'), frame_shape)\n",
    "    if mask is None:\n",
    "        x1, y1, x2, y2 = state['bbox']\n",
    "        temp = np.zeros(frame_shape, dtype=np.uint8)\n",
    "        cv2.ellipse(\n",
    "            temp,\n",
    "            center=(int((x1 + x2) / 2), int((y1 + y2) / 2)),\n",
    "            axes=(int(max((x2 - x1) / 2, 1)), int(max((y2 - y1) / 2, 1))),\n",
    "            angle=0, startAngle=0, endAngle=360,\n",
    "            color=255, thickness=-1\n",
    "        )\n",
    "        mask = temp\n",
    "    if feather > 0:\n",
    "        mask = cv2.GaussianBlur(mask, (0, 0), sigmaX=feather)\n",
    "    return np.clip(mask.astype(np.float32) / 255.0, 0, 1)[..., None]\n",
    "\n",
    "\n",
    "def clamp_window(center, scale, frame_size):\n",
    "    W, H = frame_size\n",
    "    crop_w = max(W * scale, 64)\n",
    "    crop_h = max(H * scale, 64)\n",
    "    x1 = np.clip(center[0] - crop_w / 2, 0, W - crop_w)\n",
    "    y1 = np.clip(center[1] - crop_h / 2, 0, H - crop_h)\n",
    "    x2 = x1 + crop_w\n",
    "    y2 = y1 + crop_h\n",
    "    return int(x1), int(y1), int(x2), int(y2)\n",
    "\n",
    "\n",
    "def run_moviepy(plan, frame_fn, output_path, codec='libx264'):\n",
    "    \"\"\"Render video with Premiere Pro compatible encoding.\"\"\"\n",
    "    clip = VideoFileClip(plan['video_path'])\n",
    "    sampler = timeline_sampler(plan['timeline'])\n",
    "    H = int(plan['frame_size'][1])\n",
    "    W = int(plan['frame_size'][0])\n",
    "\n",
    "    def processor(get_frame, t):\n",
    "        frame = get_frame(t)\n",
    "        if t < plan['t_in'] or t > plan['t_out']:\n",
    "            return frame\n",
    "        state = sampler(t)\n",
    "        return frame_fn(frame, state, (H, W))\n",
    "\n",
    "    processed = clip.fl(processor)\n",
    "    \n",
    "    # Get encoding params from source video\n",
    "    enc_params = get_video_encoding_params(plan['video_path'])\n",
    "    \n",
    "    if TEST_MODE:\n",
    "        # Fast preview mode\n",
    "        processed = processed.resize(height=480)\n",
    "        processed.write_videofile(\n",
    "            str(output_path),\n",
    "            codec=codec,\n",
    "            audio=True,\n",
    "            audio_codec='aac',\n",
    "            fps=clip.fps,\n",
    "            preset='ultrafast',\n",
    "            logger=None\n",
    "        )\n",
    "    else:\n",
    "        # Production mode with Premiere Pro compatibility\n",
    "        ffmpeg_params = [\n",
    "            '-pix_fmt', 'yuv420p',\n",
    "            '-profile:v', 'high',\n",
    "            '-level', '4.1',\n",
    "            '-g', str(enc_params['gop_size']),\n",
    "            '-keyint_min', str(enc_params['keyint_min']),\n",
    "            '-bf', '2',\n",
    "            '-movflags', '+faststart',\n",
    "            # Premiere Pro compatibility flags\n",
    "            '-vsync', 'cfr',                    # Force constant frame rate\n",
    "            '-video_track_timescale', '30000',  # Standard time base\n",
    "        ]\n",
    "        processed.write_videofile(\n",
    "            str(output_path),\n",
    "            codec=codec,\n",
    "            audio=True,\n",
    "            audio_codec='aac',\n",
    "            fps=clip.fps,\n",
    "            ffmpeg_params=ffmpeg_params,\n",
    "            logger=None\n",
    "        )\n",
    "    \n",
    "    clip.close()\n",
    "    processed.close()\n",
    "    return output_path\n",
    "\n",
    "\n",
    "def render_zoom_follow(plan, output):\n",
    "    W, H = plan['frame_size']\n",
    "    def fn(frame, state, shape):\n",
    "        cx, cy = state['center']\n",
    "        scale = np.clip(state['scale'], 0.2, 1.0)\n",
    "        x1, y1, x2, y2 = clamp_window((cx, cy), scale, (W, H))\n",
    "        cropped = frame[int(y1):int(y2), int(x1):int(x2)]\n",
    "        return cv2.resize(cropped, (W, H), interpolation=cv2.INTER_CUBIC)\n",
    "    return run_moviepy(plan, fn, output)\n",
    "\n",
    "\n",
    "def render_spotlight(plan, output):\n",
    "    feather = int(plan['params'].get('feather', 45))\n",
    "    strength = plan['params'].get('strength', 0.7)\n",
    "    def fn(frame, state, shape):\n",
    "        mask = ensure_mask(state, shape, feather)\n",
    "        dimmed = (frame * (1 - strength)).astype(np.uint8)\n",
    "        return (frame * mask + dimmed * (1 - mask)).astype(np.uint8)\n",
    "    return run_moviepy(plan, fn, output)\n",
    "\n",
    "\n",
    "def render_blur_background(plan, output):\n",
    "    ksize = int(plan['params'].get('ksize', 21))\n",
    "    if ksize % 2 == 0:\n",
    "        ksize += 1\n",
    "    def fn(frame, state, shape):\n",
    "        mask = ensure_mask(state, shape, 25)\n",
    "        blurred = cv2.GaussianBlur(frame, (ksize, ksize), 0)\n",
    "        return (frame * mask + blurred * (1 - mask)).astype(np.uint8)\n",
    "    return run_moviepy(plan, fn, output)\n",
    "\n",
    "\n",
    "def render_pixelate(plan, output):\n",
    "    block = int(plan['params'].get('block', 20))\n",
    "    def fn(frame, state, shape):\n",
    "        mask = ensure_mask(state, shape, 5)\n",
    "        h, w, _ = frame.shape\n",
    "        small = cv2.resize(frame, (max(1, w // block), max(1, h // block)), interpolation=cv2.INTER_LINEAR)\n",
    "        pixelated = cv2.resize(small, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "        return (frame * (1 - mask) + pixelated * mask).astype(np.uint8)\n",
    "    return run_moviepy(plan, fn, output)\n",
    "\n",
    "\n",
    "def render_callout(plan, output):\n",
    "    label = plan['params'].get('label', plan['object'])\n",
    "    def fn(frame, state, shape):\n",
    "        frame_out = frame.copy()\n",
    "        cx, cy = map(int, state['center'])\n",
    "        anchor = (np.clip(cx + 100, 0, frame.shape[1] - 1), np.clip(cy - 100, 0, frame.shape[0] - 1))\n",
    "        cv2.line(frame_out, (cx, cy), anchor, (255, 255, 255), 2)\n",
    "        cv2.circle(frame_out, (cx, cy), 6, (0, 255, 0), -1)\n",
    "        box_w, box_h = 160, 60\n",
    "        x1 = np.clip(anchor[0], 0, frame.shape[1] - box_w - 1)\n",
    "        y1 = np.clip(anchor[1], 0, frame.shape[0] - box_h - 1)\n",
    "        x2, y2 = x1 + box_w, y1 + box_h\n",
    "        overlay = frame_out.copy()\n",
    "        cv2.rectangle(overlay, (x1, y1), (x2, y2), (0, 0, 0), -1)\n",
    "        cv2.addWeighted(overlay, 0.65, frame_out, 0.35, 0, frame_out)\n",
    "        cv2.rectangle(frame_out, (x1, y1), (x2, y2), (255, 255, 255), 2)\n",
    "        cv2.putText(frame_out, label, (x1 + 12, y1 + 35), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (255, 255, 255), 2)\n",
    "        return frame_out\n",
    "    return run_moviepy(plan, fn, output)\n",
    "\n",
    "\n",
    "def render_path(plan, output):\n",
    "    points = np.array([item['center'] for item in plan['timeline']], dtype=np.int32)\n",
    "    def fn(frame, state, shape):\n",
    "        frame_out = frame.copy()\n",
    "        cv2.polylines(frame_out, [points], False, (0, 255, 255), 4)\n",
    "        return frame_out\n",
    "    return run_moviepy(plan, fn, output)\n",
    "\n",
    "\n",
    "RENDERERS = {\n",
    "    'ZoomFollow': render_zoom_follow,\n",
    "    'Spotlight': render_spotlight,\n",
    "    'BlurBackground': render_blur_background,\n",
    "    'PixelateObject': render_pixelate,\n",
    "    'Callout': render_callout,\n",
    "    'PathOverlay': render_path\n",
    "}\n",
    "\n",
    "print('‚úÖ Renderers loaded (v3 - Premiere Pro Compatible)')\n",
    "print(f'   üß™ TEST_MODE: {TEST_MODE}', '‚Üí 480p, ultrafast' if TEST_MODE else '‚Üí full quality + Premiere Pro flags')\n",
    "print(f'   Effects: {list(RENDERERS.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting ChatCut server...\n",
      "\n",
      "============================================================\n",
      "üéâ CHATCUT SERVER READY\n",
      "============================================================\n",
      "\n",
      "üì° Copy this URL into Premiere Pro:\n",
      "\n",
      "   NgrokTunnel: \"https://eac976108fd9.ngrok-free.app\" -> \"http://localhost:8000\"\n",
      "\n",
      "============================================================\n",
      "\n",
      "‚öôÔ∏è  CONFIGURATION:\n",
      "   ‚Ä¢ TEST_MODE: False (full quality)\n",
      "   ‚Ä¢ Device: CUDA\n",
      "   ‚Ä¢ Gemini: gemini-2.5-flash\n",
      "   ‚Ä¢ CLIP: ViT-L/14\n",
      "\n",
      "üîß PIPELINE:\n",
      "   1. Upload ‚Üí YOLO tracking (ByteTrack)\n",
      "   2. Parse ‚Üí Gemini NL‚ÜíDSL\n",
      "   3. Plan ‚Üí CLIP semantic match + Gemini rerank\n",
      "   4. Render ‚Üí MoviePy + OpenCV\n",
      "\n",
      "üìä AVAILABLE EFFECTS:\n",
      "   ‚Ä¢ ZoomFollow: zoom, punch in, follow\n",
      "   ‚Ä¢ Spotlight: spotlight, highlight\n",
      "   ‚Ä¢ BlurBackground: blur background, background blur\n",
      "   ‚Ä¢ PixelateObject: pixelate\n",
      "   ‚Ä¢ Callout: callout, label\n",
      "   ‚Ä¢ PathOverlay: path, trajectory\n",
      "\n",
      "üåê ENDPOINTS:\n",
      "   POST /start-job       - Async processing (returns job_id)\n",
      "   GET  /progress/{id}   - Poll progress (0-100%)\n",
      "   GET  /download/{file} - Download result\n",
      "   POST /process         - Sync endpoint (legacy)\n",
      "   GET  /health          - Health check\n",
      "   GET  /effects         - List available effects\n",
      "\n",
      "‚úÖ Server running! Logs will appear below.\n",
      "   To stop: Runtime ‚Üí Restart runtime\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì• NEW JOB RECEIVED\n",
      "   Job ID: cbc6ead4\n",
      "   File: Jay_Prakash_Guiding_at_Wikimedia_Hackathon_Kochi_2024_0.00_5.83.mp4 (3.4 MB)\n",
      "   Prompt: 'zoom in on the person in green and center them'\n",
      "[07:45:46] [Job cbc6ead4] upload: 5% - Received Jay_Prakash_Guiding_at_Wikimedia_Hackathon_Kochi_2024_0.00_5.83.mp4 (3.4 MB)\n",
      "\n",
      "######################################################################\n",
      "# JOB cbc6ead4 STARTED\n",
      "######################################################################\n",
      "   File: Jay_Prakash_Guiding_at_Wikimedia_Hackathon_Kochi_2024_0.00_5.83.mp4\n",
      "   Prompt: 'zoom in on the person in green and center them'\n",
      "   TEST_MODE: False\n",
      "   Device: cuda\n",
      "######################################################################\n",
      "\n",
      "[07:45:46] [Job cbc6ead4] tracking: 0% - Starting object tracking...\n",
      "üìπ VIDEO INFO:\n",
      "   Resolution: 608x1080\n",
      "   FPS: 30.00\n",
      "   Frames: 175\n",
      "   Duration: 5.83s\n",
      "\n",
      "üîç TRACKING PHASE:\n",
      "   Model: yolo11n-seg.pt\n",
      "   Tracker: ByteTrack\n",
      "[07:45:51] [Job cbc6ead4] tracking: 4% - Tracking frame 18/175...\n",
      "[07:45:58] [Job cbc6ead4] tracking: 9% - Tracking frame 40/175...\n",
      "[07:46:11] [Job cbc6ead4] tracking: 14% - Tracking frame 62/175...\n",
      "[07:46:26] [Job cbc6ead4] tracking: 19% - Tracking frame 84/175...\n",
      "[07:46:42] [Job cbc6ead4] tracking: 24% - Tracking frame 105/175...\n",
      "[07:46:58] [Job cbc6ead4] tracking: 29% - Tracking frame 127/175...\n",
      "[07:47:17] [Job cbc6ead4] tracking: 34% - Tracking frame 149/175...\n",
      "[07:47:37] [Job cbc6ead4] tracking: 39% - Tracking frame 171/175...\n",
      "   ‚úÖ Tracking complete in 114.9s\n",
      "   Frames processed: 175\n",
      "   Unique tracks: 64\n",
      "[07:47:41] [Job cbc6ead4] tracking: 40% - Tracked 175 frames, 64 objects\n",
      "[07:47:41] [Job cbc6ead4] parsing: 45% - Parsing command with Gemini...\n",
      "\n",
      "‚è±Ô∏è  Gemini parsing took 1.73s\n",
      "[07:47:43] [Job cbc6ead4] parsing: 50% - Found 1 effect(s): ZoomFollow\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "üé¨ PROCESSING EFFECT 1/1: ZoomFollow\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[07:47:43] [Job cbc6ead4] effect_1: 55% - [1/1] Planning ZoomFollow...\n",
      "\n",
      "============================================================\n",
      "üìä KEYFRAME PLANNER: ZoomFollow\n",
      "============================================================\n",
      "   Raw object: 'person in green'\n",
      "   Cleaned: 'a person in green'\n",
      "   CLIP query: 'a person in green, main person in the shot, framed from the waist up, clearly vi...'\n",
      "   Selection method: GEMINI ‚Üí CLIP\n",
      "   ü§ñ Gemini selected track 35\n",
      "\n",
      "   üìç FINAL PLAN:\n",
      "      Track ID: 35\n",
      "      Keyframes: 146\n",
      "      Time range: 0.00s ‚Üí 5.83s\n",
      "      Params: {'margin': 0.1}\n",
      "============================================================\n",
      "\n",
      "   Planning took 3.9s\n",
      "[07:47:47] [Job cbc6ead4] effect_1: 60% - [1/1] Rendering ZoomFollow...\n",
      "Source video: 30.00fps, GOP=30\n",
      "   ‚úÖ Rendering took 2.4s\n",
      "[07:47:49] [Job cbc6ead4] effect_1: 65% - [1/1] ZoomFollow complete (2.4s)\n",
      "\n",
      "######################################################################\n",
      "# JOB cbc6ead4 COMPLETE\n",
      "######################################################################\n",
      "   Total time: 122.9s\n",
      "   Effects applied: ZoomFollow\n",
      "   Output: /content/exports/processed_Jay_Prakash_Guiding_at_Wikimedia_Hackathon_Kochi_2024_0.00_5.83.mp4\n",
      "######################################################################\n",
      "\n",
      "[07:47:49] [Job cbc6ead4] complete: 100% - Complete in 122.9s! Applied: ZoomFollow\n"
     ]
    }
   ],
   "source": [
    "#@title 7. START SERVER (Multi-Effect + Progress Polling)\n",
    "# ‚ö†Ô∏è GET YOUR TOKEN: https://ngrok.com ‚Üí sign up ‚Üí Your Authtoken\n",
    "# Kill any existing ngrok tunnels\n",
    "try:\n",
    "    from pyngrok import ngrok\n",
    "    ngrok.kill()\n",
    "except:\n",
    "    pass\n",
    "NGROK_TOKEN = \"YOUR_NGROK_TOKEN\"  # <-- PASTE YOUR TOKEN\n",
    "\n",
    "#============================================================\n",
    "from pyngrok import ngrok\n",
    "from fastapi import FastAPI, UploadFile, File, Form, HTTPException\n",
    "from fastapi.responses import FileResponse\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "import uvicorn\n",
    "import threading\n",
    "import json as json_lib\n",
    "import uuid\n",
    "import shutil\n",
    "import time as _time\n",
    "\n",
    "if NGROK_TOKEN == \"YOUR_TOKEN_HERE\":\n",
    "    raise ValueError(\"‚ùå Paste your ngrok token above! Get it free at https://ngrok.com\")\n",
    "\n",
    "ngrok.set_auth_token(NGROK_TOKEN)\n",
    "\n",
    "app = FastAPI(title=\"ChatCut\")\n",
    "app.add_middleware(CORSMiddleware, allow_origins=[\"*\"], allow_methods=[\"*\"], allow_headers=[\"*\"])\n",
    "\n",
    "# Store completed files for download\n",
    "completed_files = {}\n",
    "\n",
    "# Job progress storage - key: job_id, value: progress dict\n",
    "job_progress = {}\n",
    "\n",
    "@app.get(\"/health\")\n",
    "def health():\n",
    "    return {\"status\": \"ok\", \"gpu\": DEVICE, \"test_mode\": TEST_MODE, \"gemini_model\": GEMINI_MODEL_ID}\n",
    "\n",
    "@app.get(\"/effects\")\n",
    "def effects():\n",
    "    return {\"effects\": list(RENDERERS.keys())}\n",
    "\n",
    "def update_progress(job_id, stage, progress, message, **extra):\n",
    "    \"\"\"Update progress for a job.\"\"\"\n",
    "    job_progress[job_id] = {\n",
    "        \"status\": \"processing\" if stage not in [\"complete\", \"error\"] else stage,\n",
    "        \"stage\": stage,\n",
    "        \"progress\": progress,\n",
    "        \"message\": message,\n",
    "        **extra\n",
    "    }\n",
    "    # Enhanced logging with timestamp\n",
    "    timestamp = _time.strftime(\"%H:%M:%S\")\n",
    "    print(f\"[{timestamp}] [Job {job_id}] {stage}: {progress}% - {message}\")\n",
    "\n",
    "def process_job(job_id, file_path, filename, prompt):\n",
    "    \"\"\"Background job to process video - supports MULTIPLE EFFECTS.\"\"\"\n",
    "    temp_files = []\n",
    "    current_video = file_path\n",
    "    job_start_time = _time.time()\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\n{'#'*70}\")\n",
    "        print(f\"# JOB {job_id} STARTED\")\n",
    "        print(f\"{'#'*70}\")\n",
    "        print(f\"   File: {filename}\")\n",
    "        print(f\"   Prompt: '{prompt}'\")\n",
    "        print(f\"   TEST_MODE: {TEST_MODE}\")\n",
    "        print(f\"   Device: {DEVICE}\")\n",
    "        print(f\"{'#'*70}\\n\")\n",
    "        \n",
    "        update_progress(job_id, \"tracking\", 0, \"Starting object tracking...\")\n",
    "        global tracks_df_cached\n",
    "        tracks_df_cached = None\n",
    "\n",
    "        # Get video info\n",
    "        cap = cv2.VideoCapture(current_video)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "        w, h = int(cap.get(3)), int(cap.get(4))\n",
    "        duration = total_frames / fps if fps else 0\n",
    "        cap.release()\n",
    "\n",
    "        print(f\"üìπ VIDEO INFO:\")\n",
    "        print(f\"   Resolution: {w}x{h}\")\n",
    "        print(f\"   FPS: {fps:.2f}\")\n",
    "        print(f\"   Frames: {total_frames}\")\n",
    "        print(f\"   Duration: {duration:.2f}s\\n\")\n",
    "\n",
    "        # Run tracking with progress updates\n",
    "        track_start = _time.time()\n",
    "        model = load_model(use_seg=True)\n",
    "        name_map = _build_name_map(model)\n",
    "\n",
    "        print(f\"üîç TRACKING PHASE:\")\n",
    "        print(f\"   Model: {DEFAULT_MODELS['seg']}\")\n",
    "        print(f\"   Tracker: ByteTrack\")\n",
    "        \n",
    "        stream = model.track(source=current_video, imgsz=960, tracker='bytetrack.yaml', stream=True,\n",
    "                            conf=0.25, iou=0.45, vid_stride=1, device=DEVICE, verbose=False, persist=True)\n",
    "\n",
    "        frames_data = []\n",
    "        cursor = 0\n",
    "        last_progress = -1\n",
    "        unique_track_ids = set()\n",
    "\n",
    "        for result in stream:\n",
    "            dets = []\n",
    "            if result.boxes is not None and result.boxes.id is not None:\n",
    "                ids = result.boxes.id.int().cpu().tolist()\n",
    "                xyxy = result.boxes.xyxy.cpu().tolist()\n",
    "                confs = result.boxes.conf.cpu().tolist()\n",
    "                clss = result.boxes.cls.int().cpu().tolist()\n",
    "                masks = result.masks.data.cpu().numpy() if result.masks else None\n",
    "                for i, tid in enumerate(ids):\n",
    "                    unique_track_ids.add(tid)\n",
    "                    dets.append({'id': int(tid), 'cls': name_map.get(clss[i], str(clss[i])),\n",
    "                                'conf': float(confs[i]), 'bbox_xyxy': [float(v) for v in xyxy[i]],\n",
    "                                'mask_rle': encode_mask(masks[i]) if masks is not None else None})\n",
    "            frames_data.append({'frame_index': cursor, 't': cursor/fps, 'detections': dets})\n",
    "            cursor += 1\n",
    "\n",
    "            track_progress = int((cursor / total_frames) * 40) if total_frames > 0 else 0\n",
    "            if track_progress >= last_progress + 5:\n",
    "                last_progress = track_progress\n",
    "                update_progress(job_id, \"tracking\", track_progress,\n",
    "                              f\"Tracking frame {cursor}/{total_frames}...\")\n",
    "\n",
    "        track_time = _time.time() - track_start\n",
    "        tracks = {\n",
    "            'video_path': current_video,\n",
    "            'fps': fps,\n",
    "            'size': [w, h],\n",
    "            'duration': duration,\n",
    "            'frames': frames_data\n",
    "        }\n",
    "\n",
    "        print(f\"   ‚úÖ Tracking complete in {track_time:.1f}s\")\n",
    "        print(f\"   Frames processed: {len(frames_data)}\")\n",
    "        print(f\"   Unique tracks: {len(unique_track_ids)}\")\n",
    "        \n",
    "        update_progress(job_id, \"tracking\", 40, f\"Tracked {len(frames_data)} frames, {len(unique_track_ids)} objects\")\n",
    "\n",
    "        # Parse command with Gemini\n",
    "        parse_start = _time.time()\n",
    "        update_progress(job_id, \"parsing\", 45, \"Parsing command with Gemini...\")\n",
    "        cmds = parse_nl_to_dsl(prompt, tracks['duration'])\n",
    "        parse_time = _time.time() - parse_start\n",
    "        \n",
    "        num_effects = len(cmds)\n",
    "        effect_names = [cmd.effect for cmd in cmds]\n",
    "        \n",
    "        print(f\"\\n‚è±Ô∏è  Gemini parsing took {parse_time:.2f}s\")\n",
    "        \n",
    "        update_progress(job_id, \"parsing\", 50, \n",
    "                       f\"Found {num_effects} effect(s): {', '.join(effect_names)}\")\n",
    "        \n",
    "        # Process each effect sequentially\n",
    "        for i, cmd in enumerate(cmds):\n",
    "            effect_num = i + 1\n",
    "            progress_base = 50 + int((i / num_effects) * 45)\n",
    "            \n",
    "            print(f\"\\n{'‚îÄ'*60}\")\n",
    "            print(f\"üé¨ PROCESSING EFFECT {effect_num}/{num_effects}: {cmd.effect}\")\n",
    "            print(f\"{'‚îÄ'*60}\")\n",
    "            \n",
    "            if i > 0:\n",
    "                tracks_df_cached = None\n",
    "                update_progress(job_id, f\"effect_{effect_num}\", progress_base, \n",
    "                              f\"[{effect_num}/{num_effects}] Re-tracking for {cmd.effect}...\")\n",
    "                \n",
    "                retrack_start = _time.time()\n",
    "                tracks_rerun = detect_and_track(current_video, use_seg=True, frame_stride=1)\n",
    "                tracks = tracks_rerun\n",
    "                print(f\"   Re-tracking took {_time.time() - retrack_start:.1f}s\")\n",
    "            \n",
    "            update_progress(job_id, f\"effect_{effect_num}\", progress_base + 5, \n",
    "                          f\"[{effect_num}/{num_effects}] Planning {cmd.effect}...\")\n",
    "            \n",
    "            plan_start = _time.time()\n",
    "            plan = plan_effect(cmd, tracks)\n",
    "            plan['video_path'] = current_video\n",
    "            print(f\"   Planning took {_time.time() - plan_start:.1f}s\")\n",
    "            \n",
    "            update_progress(job_id, f\"effect_{effect_num}\", progress_base + 10, \n",
    "                          f\"[{effect_num}/{num_effects}] Rendering {cmd.effect}...\")\n",
    "            \n",
    "            if i < num_effects - 1:\n",
    "                tmp_out = tempfile.NamedTemporaryFile(\n",
    "                    delete=False, suffix='.mp4', \n",
    "                    prefix=f'effect_{effect_num}_'\n",
    "                )\n",
    "                tmp_out.close()\n",
    "                out_path = tmp_out.name\n",
    "                temp_files.append(out_path)\n",
    "            else:\n",
    "                out_name = f\"processed_{filename}\"\n",
    "                out_path = str(EXPORT_DIR / out_name)\n",
    "            \n",
    "            effect_name = plan['effect']\n",
    "            if effect_name not in RENDERERS:\n",
    "                raise ValueError(f\"Unknown effect: {effect_name}. Available: {list(RENDERERS.keys())}\")\n",
    "            \n",
    "            render_start = _time.time()\n",
    "            RENDERERS[effect_name](plan, out_path)\n",
    "            render_time = _time.time() - render_start\n",
    "            \n",
    "            current_video = out_path\n",
    "            \n",
    "            print(f\"   ‚úÖ Rendering took {render_time:.1f}s\")\n",
    "            update_progress(job_id, f\"effect_{effect_num}\", progress_base + 15, \n",
    "                          f\"[{effect_num}/{num_effects}] {cmd.effect} complete ({render_time:.1f}s)\")\n",
    "        \n",
    "        # Final output\n",
    "        out_name = f\"processed_{filename}\"\n",
    "        final_path = str(EXPORT_DIR / out_name)\n",
    "        \n",
    "        if current_video != final_path:\n",
    "            shutil.copy2(current_video, final_path)\n",
    "\n",
    "        completed_files[out_name] = final_path\n",
    "\n",
    "        total_time = _time.time() - job_start_time\n",
    "        \n",
    "        print(f\"\\n{'#'*70}\")\n",
    "        print(f\"# JOB {job_id} COMPLETE\")\n",
    "        print(f\"{'#'*70}\")\n",
    "        print(f\"   Total time: {total_time:.1f}s\")\n",
    "        print(f\"   Effects applied: {', '.join(effect_names)}\")\n",
    "        print(f\"   Output: {final_path}\")\n",
    "        print(f\"{'#'*70}\\n\")\n",
    "        \n",
    "        update_progress(job_id, \"complete\", 100, \n",
    "                       f\"Complete in {total_time:.1f}s! Applied: {', '.join(effect_names)}\",\n",
    "                       file_ready=True,\n",
    "                       filename=out_name,\n",
    "                       output_path=final_path,\n",
    "                       effects_applied=effect_names,\n",
    "                       total_time_seconds=round(total_time, 1),\n",
    "                       download_url=f\"/download/{out_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        total_time = _time.time() - job_start_time\n",
    "        print(f\"\\n{'!'*70}\")\n",
    "        print(f\"! JOB {job_id} FAILED after {total_time:.1f}s\")\n",
    "        print(f\"{'!'*70}\")\n",
    "        print(f\"   Error: {e}\")\n",
    "        traceback.print_exc()\n",
    "        print(f\"{'!'*70}\\n\")\n",
    "        \n",
    "        update_progress(job_id, \"error\", 0, f\"Error after {total_time:.1f}s: {str(e)}\", \n",
    "                       error=str(e), total_time_seconds=round(total_time, 1))\n",
    "    finally:\n",
    "        for tmp in temp_files:\n",
    "            if os.path.exists(tmp):\n",
    "                try:\n",
    "                    os.unlink(tmp)\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        if os.path.exists(file_path) and file_path != current_video:\n",
    "            try:\n",
    "                os.unlink(file_path)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "@app.post(\"/start-job\")\n",
    "async def start_job(file: UploadFile = File(...), prompt: str = Form(...)):\n",
    "    \"\"\"Start a video processing job - returns job_id immediately.\"\"\"\n",
    "    job_id = str(uuid.uuid4())[:8]\n",
    "\n",
    "    tmp_dir = tempfile.mkdtemp()\n",
    "    file_path = os.path.join(tmp_dir, file.filename)\n",
    "\n",
    "    with open(file_path, 'wb') as f:\n",
    "        content = await file.read()\n",
    "        f.write(content)\n",
    "\n",
    "    file_size_mb = len(content) / (1024 * 1024)\n",
    "    \n",
    "    print(f\"\\nüì• NEW JOB RECEIVED\")\n",
    "    print(f\"   Job ID: {job_id}\")\n",
    "    print(f\"   File: {file.filename} ({file_size_mb:.1f} MB)\")\n",
    "    print(f\"   Prompt: '{prompt}'\")\n",
    "\n",
    "    update_progress(job_id, \"upload\", 5, f\"Received {file.filename} ({file_size_mb:.1f} MB)\")\n",
    "\n",
    "    thread = threading.Thread(\n",
    "        target=process_job,\n",
    "        args=(job_id, file_path, file.filename, prompt),\n",
    "        daemon=True\n",
    "    )\n",
    "    thread.start()\n",
    "\n",
    "    return {\n",
    "        \"job_id\": job_id,\n",
    "        \"status\": \"started\",\n",
    "        \"message\": f\"Processing started for {file.filename}\",\n",
    "        \"file_size_mb\": round(file_size_mb, 1),\n",
    "        \"test_mode\": TEST_MODE\n",
    "    }\n",
    "\n",
    "@app.get(\"/progress/{job_id}\")\n",
    "def get_progress(job_id: str):\n",
    "    \"\"\"Get progress for a job.\"\"\"\n",
    "    if job_id not in job_progress:\n",
    "        return {\"status\": \"not_found\", \"error\": f\"Job {job_id} not found\"}\n",
    "    return job_progress[job_id]\n",
    "\n",
    "@app.get(\"/download/{filename}\")\n",
    "async def download(filename: str):\n",
    "    \"\"\"Download a processed video file.\"\"\"\n",
    "    if filename not in completed_files:\n",
    "        raise HTTPException(404, f\"File not found: {filename}\")\n",
    "\n",
    "    path = completed_files[filename]\n",
    "    if not os.path.exists(path):\n",
    "        raise HTTPException(404, f\"File no longer exists: {filename}\")\n",
    "\n",
    "    return FileResponse(str(path), filename=filename, media_type=\"video/mp4\")\n",
    "\n",
    "# Keep old /process endpoint for backwards compatibility\n",
    "@app.post(\"/process\")\n",
    "async def process(file: UploadFile = File(...), prompt: str = Form(...)):\n",
    "    tmp = None\n",
    "    try:\n",
    "        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file.filename)[1] or '.mp4')\n",
    "        tmp.write(await file.read())\n",
    "        tmp.close()\n",
    "        \n",
    "        print(f\"\\nüì• SYNC REQUEST: {file.filename}\")\n",
    "        print(f\"üìù Prompt: '{prompt}'\")\n",
    "\n",
    "        global tracks_df_cached\n",
    "        tracks_df_cached = None\n",
    "        tracks = detect_and_track(tmp.name, use_seg=True, frame_stride=1)\n",
    "\n",
    "        cmds = parse_nl_to_dsl(prompt, tracks['duration'])\n",
    "        \n",
    "        current_video = tmp.name\n",
    "        temp_outputs = []\n",
    "        \n",
    "        for i, cmd in enumerate(cmds):\n",
    "            plan = plan_effect(cmd, tracks)\n",
    "            plan['video_path'] = current_video\n",
    "            \n",
    "            if i < len(cmds) - 1:\n",
    "                tmp_out = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4')\n",
    "                tmp_out.close()\n",
    "                out_path = tmp_out.name\n",
    "                temp_outputs.append(out_path)\n",
    "            else:\n",
    "                out_name = f\"processed_{file.filename}\"\n",
    "                out_path = str(EXPORT_DIR / out_name)\n",
    "            \n",
    "            RENDERERS[plan['effect']](plan, out_path)\n",
    "            current_video = out_path\n",
    "            \n",
    "            if i < len(cmds) - 1:\n",
    "                tracks_df_cached = None\n",
    "                tracks = detect_and_track(current_video, use_seg=True, frame_stride=1)\n",
    "        \n",
    "        print(f\"‚úÖ Sync request complete: {out_path}\")\n",
    "\n",
    "        for tmp_out in temp_outputs:\n",
    "            if os.path.exists(tmp_out):\n",
    "                os.unlink(tmp_out)\n",
    "\n",
    "        return FileResponse(str(out_path), filename=out_name, media_type=\"video/mp4\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Sync request failed: {e}\")\n",
    "        traceback.print_exc()\n",
    "        raise HTTPException(500, str(e))\n",
    "    finally:\n",
    "        if tmp and os.path.exists(tmp.name): os.unlink(tmp.name)\n",
    "\n",
    "# Start server\n",
    "def run_server():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"warning\")\n",
    "\n",
    "print(\"üöÄ Starting ChatCut server...\")\n",
    "thread = threading.Thread(target=run_server, daemon=True)\n",
    "thread.start()\n",
    "\n",
    "import time\n",
    "time.sleep(2)\n",
    "\n",
    "url = ngrok.connect(8000)\n",
    "print(\"\")\n",
    "print(\"=\" * 60)\n",
    "print(\"üéâ CHATCUT SERVER READY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\")\n",
    "print(f\"üì° Copy this URL into Premiere Pro:\")\n",
    "print(f\"\")\n",
    "print(f\"   {url}\")\n",
    "print(f\"\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\")\n",
    "print(\"‚öôÔ∏è  CONFIGURATION:\")\n",
    "print(f\"   ‚Ä¢ TEST_MODE: {TEST_MODE}\" + (\" (480p, ultrafast)\" if TEST_MODE else \" (full quality)\"))\n",
    "print(f\"   ‚Ä¢ Device: {DEVICE.upper()}\")\n",
    "print(f\"   ‚Ä¢ Gemini: {GEMINI_MODEL_ID}\")\n",
    "print(f\"   ‚Ä¢ CLIP: ViT-L/14\")\n",
    "print(\"\")\n",
    "print(\"üîß PIPELINE:\")\n",
    "print(\"   1. Upload ‚Üí YOLO tracking (ByteTrack)\")\n",
    "print(\"   2. Parse ‚Üí Gemini NL‚ÜíDSL\")\n",
    "print(\"   3. Plan ‚Üí CLIP semantic match + Gemini rerank\")\n",
    "print(\"   4. Render ‚Üí MoviePy + OpenCV\")\n",
    "print(\"\")\n",
    "print(\"üìä AVAILABLE EFFECTS:\")\n",
    "for eff in RENDERERS.keys():\n",
    "    keywords = EFFECT_KEYWORDS.get(eff, [])\n",
    "    print(f\"   ‚Ä¢ {eff}: {', '.join(keywords)}\")\n",
    "print(\"\")\n",
    "print(\"üåê ENDPOINTS:\")\n",
    "print(\"   POST /start-job       - Async processing (returns job_id)\")\n",
    "print(\"   GET  /progress/{id}   - Poll progress (0-100%)\")\n",
    "print(\"   GET  /download/{file} - Download result\")\n",
    "print(\"   POST /process         - Sync endpoint (legacy)\")\n",
    "print(\"   GET  /health          - Health check\")\n",
    "print(\"   GET  /effects         - List available effects\")\n",
    "print(\"\")\n",
    "print(\"‚úÖ Server running! Logs will appear below.\")\n",
    "print(\"   To stop: Runtime ‚Üí Restart runtime\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
