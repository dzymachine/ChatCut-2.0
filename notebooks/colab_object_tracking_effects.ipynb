{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5d732bbc",
      "metadata": {
        "id": "5d732bbc"
      },
      "source": [
        "\n",
        "# Premiere-Style Object Tracking Effects (Colab)\n",
        "This notebook downloads CC0/public-domain demo clips, runs Ultralytics YOLO + ByteTrack/BoT-SORT, parses natural-language effect prompts into a tiny DSL, plans keyframes, and renders multiple object-aware video effects (zoom/follow, spotlight, blur, reframe, callouts, PiP magnifier, trajectory overlay). It is organized so teammates can later wrap the same functions behind a UXP-friendly service layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4076adc",
      "metadata": {
        "id": "b4076adc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71318f1d-9178-4fce-fb39-a692d5096661"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "\n",
        "#@title Run Me (installs runtime dependencies)\n",
        "!pip install -q ultralytics opencv-python moviepy numpy pandas tqdm matplotlib scipy ipywidgets fastapi uvicorn python-multipart lapx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f881336",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f881336",
        "outputId": "ae0cfee8-e3b5-423e-8770-54547a18c258"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/moviepy/config_defaults.py:47: SyntaxWarning: invalid escape sequence '\\P'\n",
            "  IMAGEMAGICK_BINARY = r\"C:\\Program Files\\ImageMagick-6.8.8-Q16\\magick.exe\"\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:294: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  lines_video = [l for l in lines if ' Video: ' in l and re.search('\\d+x\\d+', l)]\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:367: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  rotation_lines = [l for l in lines if 'rotate          :' in l and re.search('\\d+$', l)]\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:370: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  match = re.search('\\d+$', rotation_line)\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Using device: CUDA | torch 2.8.0+cu126\n",
            "Exports directory: /content/exports\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#@title Imports & global config\n",
        "from __future__ import annotations\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import textwrap\n",
        "from dataclasses import dataclass, field, asdict\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple, Any\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from scipy.signal import savgol_filter\n",
        "import matplotlib.pyplot as plt\n",
        "from moviepy.editor import VideoFileClip\n",
        "from IPython.display import Video, display\n",
        "import torch\n",
        "import ipywidgets as widgets\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "plt.style.use(\"ggplot\")\n",
        "\n",
        "BASE_DIR = Path('/content')\n",
        "EXPORT_DIR = BASE_DIR / 'exports'\n",
        "EXPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "CLIP_PATHS = {\n",
        "    'clipA': BASE_DIR / 'clipA.mp4'\n",
        "}\n",
        "DEFAULT_MODELS = {'det': 'yolo11n.pt', 'seg': 'yolo11n-seg.pt'}\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {DEVICE.upper()} | torch {torch.__version__}\")\n",
        "if DEVICE == 'cpu':\n",
        "    print('⚠️ GPU not detected. Enable a T4/other GPU runtime for best performance.')\n",
        "\n",
        "EASINGS = {\n",
        "    'linear': lambda x: x,\n",
        "    'ease': lambda x: 3 * x**2 - 2 * x**3,\n",
        "    'ease-in': lambda x: x**2,\n",
        "    'ease-out': lambda x: 1 - (1 - x)**2,\n",
        "    'ease-in-out': lambda x: (math.cos(math.pi + math.pi * x) + 1) / 2\n",
        "}\n",
        "\n",
        "SAVGOL_DEFAULT = {'window_length': 9, 'polyorder': 2}\n",
        "\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "print('Exports directory:', EXPORT_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45d52664",
      "metadata": {
        "id": "45d52664"
      },
      "source": [
        "\n",
        "## Download CC0 / Public Domain demo clips\n",
        "We pull two Wikimedia Commons clips that ship with explicit `{{cc-zero}}` licensing:\n",
        "\n",
        "* **clipA** – `Jay_Prakash_Guiding_at_Wikimedia_Hackathon_Kochi_2024.webm` (speaker + participants)\n",
        "\n",
        "Both are transcoded to MP4 (≤720p) and saved under `/content`. Licenses are logged for auditability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "642c1694",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "642c1694",
        "outputId": "a4e02e66-9a78-4f8e-f5ff-d5a775d25bb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://upload.wikimedia.org/wikipedia/commons/c/c5/Jay_Prakash_Guiding_at_Wikimedia_Hackathon_Kochi_2024.webm → clipA.mp4.dl\n",
            "Transcoding clipA.mp4.dl (5.83s) → clipA.mp4\n",
            "Saved manifest → /content/exports/clip_manifest.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'path': '/content/clipA.mp4',\n",
              "  'duration': 5.83,\n",
              "  'size': [608, 1080],\n",
              "  'credit': 'Wikimedia Commons · CC0 1.0'}]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "#@title Download demo clips (idempotent)\n",
        "import urllib.request\n",
        "from urllib.error import HTTPError\n",
        "import requests\n",
        "\n",
        "CLIP_SOURCES = {\n",
        "    'clipA.mp4': {\n",
        "        'url': 'https://upload.wikimedia.org/wikipedia/commons/c/c5/Jay_Prakash_Guiding_at_Wikimedia_Hackathon_Kochi_2024.webm',\n",
        "        'credit': 'Wikimedia Commons · CC0 1.0',\n",
        "        'convert_from': 'webm'\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "def _download_file(url: str, target: Path) -> Path:\n",
        "    target.parent.mkdir(parents=True, exist_ok=True)\n",
        "    tmp_path = target.with_suffix(target.suffix + '.dl')\n",
        "    if tmp_path.exists():\n",
        "        tmp_path.unlink()\n",
        "    print(f'Downloading {url} → {tmp_path.name}')\n",
        "    try:\n",
        "        # Use requests with a custom User-Agent\n",
        "        headers = {\"User-Agent\": \"Mozilla/5.0 (Colab demo)\"}\n",
        "        response = requests.get(url, stream=True, headers=headers)\n",
        "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "        with open(tmp_path, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error downloading {url}: {e}\")\n",
        "        raise e\n",
        "    return tmp_path\n",
        "\n",
        "\n",
        "def _transcode_to_mp4(src: Path, dst: Path, max_width: int = 1280):\n",
        "    clip = VideoFileClip(str(src))\n",
        "    resized = clip.resize(width=max_width) if clip.w > max_width else clip\n",
        "    print(f'Transcoding {src.name} ({clip.duration:.2f}s) → {dst.name}')\n",
        "    resized.write_videofile(\n",
        "        str(dst),\n",
        "        codec='libx264',\n",
        "        audio=clip.audio is not None,\n",
        "        audio_codec='aac',\n",
        "        bitrate='4M',\n",
        "        fps=clip.fps or 24,\n",
        "        logger=None\n",
        "    )\n",
        "    clip.close()\n",
        "    if resized is not clip:\n",
        "        resized.close()\n",
        "\n",
        "\n",
        "def download_demo_clips(force: bool = False):\n",
        "    manifest = []\n",
        "    for filename, meta in CLIP_SOURCES.items():\n",
        "        target = BASE_DIR / filename\n",
        "        if target.exists() and not force:\n",
        "            clip = VideoFileClip(str(target))\n",
        "            manifest.append({'path': str(target), 'duration': clip.duration, 'size': [clip.w, clip.h], 'credit': meta['credit']})\n",
        "            clip.close()\n",
        "            continue\n",
        "        tmp = _download_file(meta['url'], target)\n",
        "        if meta.get('convert_from'):\n",
        "            _transcode_to_mp4(tmp, target)\n",
        "            tmp.unlink(missing_ok=True)\n",
        "        else:\n",
        "            shutil.move(tmp, target)\n",
        "        clip = VideoFileClip(str(target))\n",
        "        manifest.append({'path': str(target), 'duration': clip.duration, 'size': [clip.w, clip.h], 'credit': meta['credit']})\n",
        "        clip.close()\n",
        "    manifest_path = EXPORT_DIR / 'clip_manifest.json'\n",
        "    with open(manifest_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(manifest, f, indent=2)\n",
        "    print('Saved manifest →', manifest_path)\n",
        "    return manifest\n",
        "\n",
        "clip_manifest = download_demo_clips(force=False)\n",
        "clip_manifest"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfc0d322",
      "metadata": {
        "id": "bfc0d322"
      },
      "source": [
        "\n",
        "## Detect & Track (YOLO + ByteTrack/BoT-SORT)\n",
        "This section exposes a service-like API:\n",
        "```\n",
        "detect_and_track(video_path, classes=None, tracker='bytetrack', use_seg=False)\n",
        "```\n",
        "It returns structured frame/track metadata and persists to `/content/exports/tracks.json`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a670245d",
      "metadata": {
        "id": "a670245d"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Tracking utilities\n",
        "MODEL_CACHE: Dict[str, YOLO] = {}\n",
        "CLASS_NAME_CACHE: Dict[int, Dict[int, str]] = {}\n",
        "\n",
        "\n",
        "def load_model(model_name: Optional[str] = None, use_seg: bool = False) -> YOLO:\n",
        "    name = model_name or (DEFAULT_MODELS['seg'] if use_seg else DEFAULT_MODELS['det'])\n",
        "    if name not in MODEL_CACHE:\n",
        "        print(f'Loading model {name} → {DEVICE}')\n",
        "        MODEL_CACHE[name] = YOLO(name)\n",
        "    return MODEL_CACHE[name]\n",
        "\n",
        "\n",
        "def _build_name_map(model: YOLO) -> Dict[int, str]:\n",
        "    key = id(model)\n",
        "    if key in CLASS_NAME_CACHE:\n",
        "        return CLASS_NAME_CACHE[key]\n",
        "    names = getattr(model, 'names', None)\n",
        "    if names is None and hasattr(model, 'model'):\n",
        "        names = getattr(model.model, 'names', None)\n",
        "    mapping = {}\n",
        "    if isinstance(names, dict):\n",
        "        mapping = {int(k): str(v) for k, v in names.items()}\n",
        "    elif isinstance(names, list):\n",
        "        mapping = {idx: str(val) for idx, val in enumerate(names)}\n",
        "    CLASS_NAME_CACHE[key] = mapping\n",
        "    return mapping\n",
        "\n",
        "\n",
        "def _normalize_label(text: str) -> str:\n",
        "    return text.strip().lower()\n",
        "\n",
        "\n",
        "def _class_ids_from_names(model: YOLO, classes: Optional[List[str]]):\n",
        "    if not classes:\n",
        "        return None\n",
        "    name_map = _build_name_map(model)\n",
        "    inv = {v.lower(): k for k, v in name_map.items()}\n",
        "    ids = []\n",
        "    for item in classes:\n",
        "        key = str(item).lower()\n",
        "        if key.isdigit():\n",
        "            ids.append(int(key))\n",
        "        elif key in inv:\n",
        "            ids.append(inv[key])\n",
        "    return ids or None\n",
        "\n",
        "\n",
        "def encode_mask(mask: np.ndarray) -> Dict[str, Any]:\n",
        "    mask = (mask > 0.5).astype(np.uint8)\n",
        "    pixels = mask.flatten(order='F')\n",
        "    counts = []\n",
        "    last_val = 0\n",
        "    run = 0\n",
        "    for val in pixels:\n",
        "        if val == last_val:\n",
        "            run += 1\n",
        "        else:\n",
        "            counts.append(run)\n",
        "            run = 1\n",
        "            last_val = val\n",
        "    counts.append(run)\n",
        "    if pixels.size and pixels[0] == 1:\n",
        "        counts = [0] + counts\n",
        "    return {'size': [int(mask.shape[0]), int(mask.shape[1])], 'counts': counts}\n",
        "\n",
        "\n",
        "def decode_mask(rle: Dict[str, Any]) -> np.ndarray:\n",
        "    h, w = rle['size']\n",
        "    counts = rle['counts']\n",
        "    vals = []\n",
        "    cur = 0\n",
        "    for c in counts:\n",
        "        vals.extend([cur] * c)\n",
        "        cur = 1 - cur\n",
        "    arr = np.array(vals, dtype=np.uint8)\n",
        "    return arr.reshape((h, w), order='F')\n",
        "\n",
        "\n",
        "def detect_and_track(\n",
        "    video_path: str,\n",
        "    classes: Optional[List[str]] = None,\n",
        "    tracker: str = 'bytetrack',\n",
        "    conf: float = 0.25,\n",
        "    iou: float = 0.45,\n",
        "    use_seg: bool = False,\n",
        "    frame_stride: int = 1,\n",
        "    imgsz: int = 960,\n",
        "    max_width: int = 960,\n",
        "    save_json: bool = True\n",
        ") -> Dict[str, Any]:\n",
        "    video_path = str(video_path)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH) or 0)\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) or 0)\n",
        "    duration = total_frames / fps if fps else 0\n",
        "    cap.release()\n",
        "\n",
        "    model = load_model(use_seg=use_seg)\n",
        "    class_ids = _class_ids_from_names(model, classes)\n",
        "    tracker_cfg = tracker if tracker.endswith('.yaml') else f'{tracker}.yaml'\n",
        "    effective_imgsz = min(imgsz, max_width)\n",
        "\n",
        "    print(f'Tracking {video_path} @ {fps:.2f} fps | {width}x{height} | stride={frame_stride}')\n",
        "    stream = model.track(\n",
        "        source=video_path,\n",
        "        imgsz=effective_imgsz,\n",
        "        tracker=tracker_cfg,\n",
        "        stream=True,\n",
        "        conf=conf,\n",
        "        iou=iou,\n",
        "        vid_stride=max(frame_stride, 1),\n",
        "        device=DEVICE,\n",
        "        classes=class_ids,\n",
        "        verbose=False,\n",
        "        persist=True\n",
        "    )\n",
        "\n",
        "    frames: List[Dict[str, Any]] = []\n",
        "    frame_cursor = 0\n",
        "    name_map = _build_name_map(model)\n",
        "\n",
        "    for result in tqdm(stream, desc='YOLO tracking', total=math.ceil(total_frames / max(frame_stride, 1)) or None):\n",
        "        detections = []\n",
        "        boxes = result.boxes\n",
        "        if boxes is not None and boxes.id is not None:\n",
        "            ids = boxes.id.int().cpu().tolist()\n",
        "            xyxy = boxes.xyxy.cpu().tolist()\n",
        "            confs = boxes.conf.cpu().tolist()\n",
        "            clss = boxes.cls.int().cpu().tolist()\n",
        "            mask_data = result.masks.data.cpu().numpy() if use_seg and result.masks is not None else None\n",
        "            for i, track_id in enumerate(ids):\n",
        "                bbox = [float(v) for v in xyxy[i]]\n",
        "                cls_name = name_map.get(int(clss[i]), str(clss[i]))\n",
        "                payload = encode_mask(mask_data[i]) if mask_data is not None else None\n",
        "                detections.append({\n",
        "                    'id': int(track_id),\n",
        "                    'cls': cls_name,\n",
        "                    'conf': float(confs[i]),\n",
        "                    'bbox_xyxy': bbox,\n",
        "                    'mask_rle': payload\n",
        "                })\n",
        "        frames.append({\n",
        "            'frame_index': int(frame_cursor),\n",
        "            't': float(frame_cursor / fps) if fps else 0.0,\n",
        "            'detections': detections\n",
        "        })\n",
        "        frame_cursor += max(frame_stride, 1)\n",
        "\n",
        "    payload = {\n",
        "        'video_path': video_path,\n",
        "        'fps': fps,\n",
        "        'size': [width, height],\n",
        "        'duration': duration,\n",
        "        'tracker': tracker_cfg,\n",
        "        'use_seg': use_seg,\n",
        "        'frame_stride': frame_stride,\n",
        "        'frames': frames\n",
        "    }\n",
        "    if save_json:\n",
        "        tracks_path = EXPORT_DIR / 'tracks.json'\n",
        "        with open(tracks_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(payload, f)\n",
        "        print('Saved tracks →', tracks_path)\n",
        "    return payload\n",
        "\n",
        "\n",
        "def load_tracks(json_path: Optional[Path] = None) -> Dict[str, Any]:\n",
        "    path = json_path or (EXPORT_DIR / 'tracks.json')\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        return json.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ff1a754",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ff1a754",
        "outputId": "9bea34cc-83c5-4bf5-ef1f-e479731d4c72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model yolo11n-seg.pt → cuda\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-seg.pt to 'yolo11n-seg.pt': 100% ━━━━━━━━━━━━ 5.9MB 92.3MB/s 0.1s\n",
            "Tracking /content/clipA.mp4 @ 30.00 fps | 608x1080 | stride=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLO tracking: 100%|██████████| 175/175 [02:18<00:00,  1.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved tracks → /content/exports/tracks.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'frames_with_detections': 175, 'total_frames': 175}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "\n",
        "#@title Run detection on clipA (set force_run=True to re-run)\n",
        "force_run = False\n",
        "tracks_path = EXPORT_DIR / 'tracks.json'\n",
        "if tracks_path.exists() and not force_run:\n",
        "    tracks_data = load_tracks(tracks_path)\n",
        "    print('Loaded cached tracks.json')\n",
        "else:\n",
        "    tracks_data = detect_and_track(\n",
        "        video_path=str(CLIP_PATHS['clipA']),\n",
        "        classes=None,\n",
        "        tracker='bytetrack',\n",
        "        use_seg=True,\n",
        "        frame_stride=1,\n",
        "        imgsz=960,\n",
        "        conf=0.25,\n",
        "        iou=0.45\n",
        "    )\n",
        "summary = {\n",
        "    'frames_with_detections': sum(1 for f in tracks_data['frames'] if f['detections']),\n",
        "    'total_frames': len(tracks_data['frames'])\n",
        "}\n",
        "summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7de96020",
      "metadata": {
        "id": "7de96020"
      },
      "source": [
        "\n",
        "## NL → Effect DSL\n",
        "Rule-based parser with optional LLM fallback. The DSL covers:\n",
        "* `ZoomFollow`\n",
        "* `Spotlight`\n",
        "* `BlurBackground`\n",
        "* `PixelateObject`\n",
        "* `AutoReframe`\n",
        "* `Callout`\n",
        "* `PiPMagnifier`\n",
        "* `PathOverlay`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3886e7cc",
      "metadata": {
        "id": "3886e7cc"
      },
      "outputs": [],
      "source": [
        "#@title Effect DSL parser\n",
        "EFFECT_DEFAULTS = {\n",
        "    'ZoomFollow': {'margin': 0.10, 'easing': 'ease-in-out'},\n",
        "    'Spotlight': {'strength': 0.7, 'feather': 45},\n",
        "    'BlurBackground': {'ksize': 21},\n",
        "    'PixelateObject': {'block': 20},\n",
        "    'AutoReframe': {'aspect': '9:16', 'safe': 0.8},\n",
        "    'Callout': {'label': 'object'},\n",
        "    'PiPMagnifier': {'scale': 1.5, 'radius': 120},\n",
        "    'PathOverlay': {}\n",
        "}\n",
        "\n",
        "EFFECT_KEYWORDS = {\n",
        "    'ZoomFollow': ['zoom', 'punch in', 'follow'],\n",
        "    'Spotlight': ['spotlight', 'highlight'],\n",
        "    'BlurBackground': ['blur background', 'background blur'],\n",
        "    'PixelateObject': ['pixelate', 'pixelation'],\n",
        "    'AutoReframe': ['auto-reframe', 'reframe', 'vertical'],\n",
        "    'Callout': ['callout', 'label'],\n",
        "    'PiPMagnifier': ['pip', 'magnifier', 'picture in picture'],\n",
        "    'PathOverlay': ['path', 'trajectory', 'trace']\n",
        "}\n",
        "\n",
        "@dataclass\n",
        "class EffectCommand:\n",
        "    effect: str\n",
        "    object: str\n",
        "    t_in: float\n",
        "    t_out: float\n",
        "    params: Dict[str, Any] = field(default_factory=dict)\n",
        "\n",
        "\n",
        "def _parse_time_window(text: str, video_duration: float) -> Tuple[float, float]:\n",
        "    match = re.search(r'from\\s*(\\d+(?:\\.\\d+)?)\\s*s?\\s*(?:to|-)\\s*(\\d+(?:\\.\\d+)?)', text)\n",
        "    if match:\n",
        "        return float(match.group(1)), float(match.group(2))\n",
        "    match = re.search(r'for\\s*(\\d+(?:\\.\\d+)?)\\s*s', text)\n",
        "    if match:\n",
        "        dur = float(match.group(1))\n",
        "        return 0.0, min(video_duration, dur)\n",
        "    return 0.0, video_duration\n",
        "\n",
        "\n",
        "def _extract_object(text: str) -> str:\n",
        "    lowered = text.lower()\n",
        "    sanitized = re.sub(r'[^a-z0-9 _:%-]', ' ', lowered)\n",
        "    lookahead = r'(?=\\s+(?:from|to|for|with|at|in|centered|label|,|and|$))'\n",
        "    verb_patterns = [\n",
        "        r'(?:zoom|follow|track|spotlight|highlight|blur|pixelate|callout|label|keep|keeping|focus|reframe|auto-reframe|pip|magnifier|path|trace)[^a-z0-9]+the\\s+([a-z0-9 _-]+?)' + lookahead,\n",
        "        r'around\\s+the\\s+([a-z0-9 _-]+?)' + lookahead,\n",
        "        r'keeping\\s+the\\s+([a-z0-9 _-]+)',\n",
        "        r'keep\\s+the\\s+([a-z0-9 _-]+)',\n",
        "        r'the\\s+([a-z0-9 _-]+)\\s+centered'\n",
        "    ]\n",
        "    for pattern in verb_patterns:\n",
        "        match = re.search(pattern, sanitized)\n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "    fallback = re.search(r'the\\s+([a-z0-9 _-]+?)' + lookahead, sanitized)\n",
        "    if fallback:\n",
        "        return fallback.group(1).strip()\n",
        "    centered = re.search(r'([a-z0-9 _-]+)\\s+centered', sanitized)\n",
        "    if centered:\n",
        "        return centered.group(1).strip()\n",
        "    if ' from ' in sanitized and ' the ' in sanitized.split(' from ')[0]:\n",
        "        chunk = sanitized.split(' from ')[0]\n",
        "        return chunk.split(' the ')[-1].strip()\n",
        "    tokens = [tok for tok in sanitized.split() if tok not in {'from','to','with','and','for','centered'}]\n",
        "    return tokens[-1] if tokens else 'object'\n",
        "\n",
        "\n",
        "def _extract_numeric(text: str, keyword: str, scale: float = 1.0, default: Optional[float] = None) -> Optional[float]:\n",
        "    pattern = rf'{keyword}[^0-9]*(\\d+(?:\\.\\d+)?)'\n",
        "    match = re.search(pattern, text)\n",
        "    return float(match.group(1)) * scale if match else default\n",
        "\n",
        "\n",
        "def _detect_effect(text: str) -> Optional[str]:\n",
        "    lowered = text.lower()\n",
        "    for effect, keywords in EFFECT_KEYWORDS.items():\n",
        "        if any(k in lowered for k in keywords):\n",
        "            return effect\n",
        "    return None\n",
        "\n",
        "\n",
        "def parse_nl_to_dsl(command: str, video_duration: float, fallback_llm: bool = False, llm_fn=None) -> List[EffectCommand]:\n",
        "    commands = []\n",
        "    for chunk in re.split(r'[;]\\s*\\n*', command):\n",
        "        chunk = chunk.strip()\n",
        "        if not chunk:\n",
        "            continue\n",
        "        effect = _detect_effect(chunk)\n",
        "        if effect is None:\n",
        "            if fallback_llm and llm_fn:\n",
        "                return llm_fn(chunk)\n",
        "            raise ValueError(f'Unknown effect: {chunk}')\n",
        "        t_in, t_out = _parse_time_window(chunk, video_duration)\n",
        "        obj = _extract_object(chunk)\n",
        "        params = dict(EFFECT_DEFAULTS.get(effect, {}))\n",
        "        if effect == 'ZoomFollow':\n",
        "            margin = _extract_numeric(chunk, 'margin', scale=0.01)\n",
        "            if margin is not None:\n",
        "                params['margin'] = margin\n",
        "        if effect == 'Spotlight':\n",
        "            strength = _extract_numeric(chunk, 'strength', default=None)\n",
        "            if strength is not None:\n",
        "                params['strength'] = min(max(strength, 0.1), 0.95)\n",
        "            feather = _extract_numeric(chunk, 'feather', default=None)\n",
        "            if feather is not None:\n",
        "                params['feather'] = int(feather)\n",
        "        if effect == 'BlurBackground':\n",
        "            ksize = _extract_numeric(chunk, 'blur', default=None)\n",
        "            if ksize is not None:\n",
        "                ksize = int(ksize)\n",
        "                params['ksize'] = ksize if ksize % 2 else ksize + 1\n",
        "        if effect == 'PixelateObject':\n",
        "            block = _extract_numeric(chunk, 'block', default=None)\n",
        "            if block is not None:\n",
        "                params['block'] = max(4, int(block))\n",
        "        if effect == 'AutoReframe':\n",
        "            aspect = re.search(r'(\\d+:\\d+)', chunk)\n",
        "            if aspect:\n",
        "                params['aspect'] = aspect.group(1)\n",
        "            safe = _extract_numeric(chunk, 'safe', default=None)\n",
        "            if safe is not None:\n",
        "                params['safe'] = min(max(safe, 0.1), 0.95)\n",
        "        if effect == 'Callout':\n",
        "            label = re.search(r'label\\s*([a-z0-9 _-]+)', chunk)\n",
        "            if label:\n",
        "                params['label'] = label.group(1).strip()\n",
        "        if effect == 'PiPMagnifier':\n",
        "            scale = _extract_numeric(chunk, 'scale', default=None)\n",
        "            radius = _extract_numeric(chunk, 'radius', default=None)\n",
        "            if scale is not None:\n",
        "                params['scale'] = max(1.1, scale)\n",
        "            if radius is not None:\n",
        "                params['radius'] = int(radius)\n",
        "        commands.append(EffectCommand(effect=effect, object=obj, t_in=t_in, t_out=t_out, params=params))\n",
        "    return commands\n",
        "\n",
        "LLM_PROMPT = \"\"\"You are a function that converts a natural-language prompt into the following JSON DSL:\n",
        "[{\"effect\":str, \"object\":str, \"t_in\":float, \"t_out\":float, \"params\":{...}}]\n",
        "Only emit valid JSON.\n",
        "Effects: ZoomFollow, Spotlight, BlurBackground, PixelateObject, AutoReframe, Callout, PiPMagnifier, PathOverlay.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17e8e104",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17e8e104",
        "outputId": "f464ca8f-0d04-4bb7-b1dc-171e1e222ef1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_auto_reframe (__main__.DSLParserTests.test_auto_reframe) ... ok\n",
            "test_blur_background (__main__.DSLParserTests.test_blur_background) ... ok\n",
            "test_zoom_follow (__main__.DSLParserTests.test_zoom_follow) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 3 tests in 0.010s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=3 errors=0 failures=0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "\n",
        "#@title Parser unit tests\n",
        "import unittest\n",
        "\n",
        "class DSLParserTests(unittest.TestCase):\n",
        "    def test_zoom_follow(self):\n",
        "        cmd = parse_nl_to_dsl('zoom on the person from 1.0s to 3.5s with 10% margin', video_duration=8.0)[0]\n",
        "        self.assertEqual(cmd.effect, 'ZoomFollow')\n",
        "        self.assertAlmostEqual(cmd.t_in, 1.0)\n",
        "        self.assertAlmostEqual(cmd.t_out, 3.5)\n",
        "        self.assertAlmostEqual(cmd.params['margin'], 0.10)\n",
        "\n",
        "    def test_blur_background(self):\n",
        "        cmd = parse_nl_to_dsl('blur background around the car from 0 to 5s', video_duration=8.0)[0]\n",
        "        self.assertEqual(cmd.effect, 'BlurBackground')\n",
        "        self.assertEqual(cmd.object, 'car')\n",
        "        self.assertEqual(cmd.t_out, 5.0)\n",
        "\n",
        "    def test_auto_reframe(self):\n",
        "        cmd = parse_nl_to_dsl('auto-reframe 9:16 keeping the dog centered from 2s to 7s', video_duration=10.0)[0]\n",
        "        self.assertEqual(cmd.effect, 'AutoReframe')\n",
        "        self.assertEqual(cmd.params['aspect'], '9:16')\n",
        "        self.assertEqual(cmd.object, 'dog')\n",
        "\n",
        "suite = unittest.defaultTestLoader.loadTestsFromTestCase(DSLParserTests)\n",
        "unittest.TextTestRunner(verbosity=2).run(suite)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21ce7fa2",
      "metadata": {
        "id": "21ce7fa2"
      },
      "source": [
        "\n",
        "## Keyframe planner (dominant track + smoothing)\n",
        "Pick the dominant track ID, smooth center/scale with SavGol (fallback EMA), interpolate timeline, and persist `/content/exports/keyframes.json`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae77ab6b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae77ab6b",
        "outputId": "8b44a062-67ce-48da-ceb1-0808b54829fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved keyframes → /content/exports/keyframes.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "\n",
        "#@title Keyframe utilities\n",
        "tracks_df_cached: Optional[pd.DataFrame] = None\n",
        "\n",
        "\n",
        "def tracks_to_dataframe(tracks: Dict[str, Any]) -> pd.DataFrame:\n",
        "    records = []\n",
        "    for frame in tracks['frames']:\n",
        "        for det in frame['detections']:\n",
        "            x1, y1, x2, y2 = det['bbox_xyxy']\n",
        "            records.append({\n",
        "                'frame_index': frame['frame_index'],\n",
        "                't': frame['t'],\n",
        "                'id': det['id'],\n",
        "                'cls': _normalize_label(det['cls']),\n",
        "                'conf': det['conf'],\n",
        "                'x1': x1,\n",
        "                'y1': y1,\n",
        "                'x2': x2,\n",
        "                'y2': y2,\n",
        "                'mask_rle': det.get('mask_rle')\n",
        "            })\n",
        "    return pd.DataFrame.from_records(records)\n",
        "\n",
        "\n",
        "def get_tracks_df(tracks: Dict[str, Any]) -> pd.DataFrame:\n",
        "    global tracks_df_cached\n",
        "    if tracks_df_cached is None:\n",
        "        tracks_df_cached = tracks_to_dataframe(tracks)\n",
        "    return tracks_df_cached\n",
        "\n",
        "\n",
        "def choose_track_id(df: pd.DataFrame, cls_name: str, t_in: float, t_out: float) -> Optional[int]:\n",
        "    if df.empty:\n",
        "        return None\n",
        "    window = df[(df['cls'] == _normalize_label(cls_name)) & (df['t'] >= t_in) & (df['t'] <= t_out)]\n",
        "    if window.empty:\n",
        "        return None\n",
        "    counts = window.groupby('id')['conf'].agg(['count', 'median']).reset_index()\n",
        "    counts = counts.sort_values(['count', 'median'], ascending=False)\n",
        "    return int(counts.iloc[0]['id'])\n",
        "\n",
        "\n",
        "def smooth_series(values: np.ndarray, window_length: int = 9, polyorder: int = 2) -> np.ndarray:\n",
        "    values = np.asarray(values)\n",
        "    if len(values) < window_length:\n",
        "        alpha = 0.25\n",
        "        smoothed = [values[0]]\n",
        "        for val in values[1:]:\n",
        "            smoothed.append(alpha * val + (1 - alpha) * smoothed[-1])\n",
        "        return np.array(smoothed)\n",
        "    return savgol_filter(values, window_length=window_length, polyorder=polyorder)\n",
        "\n",
        "\n",
        "def plan_effect(cmd: EffectCommand, tracks: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    df = get_tracks_df(tracks)\n",
        "    track_id = choose_track_id(df, cmd.object, cmd.t_in, cmd.t_out)\n",
        "    if track_id is None:\n",
        "        raise ValueError(f'No track found for {cmd.object}')\n",
        "    window = df[(df['id'] == track_id) & (df['t'] >= cmd.t_in) & (df['t'] <= cmd.t_out)].copy()\n",
        "    if window.empty:\n",
        "        raise ValueError('Track data missing for chosen ID')\n",
        "    centers_x = (window['x1'].values + window['x2'].values) / 2\n",
        "    centers_y = (window['y1'].values + window['y2'].values) / 2\n",
        "    widths = (window['x2'].values - window['x1'].values)\n",
        "    heights = (window['y2'].values - window['y1'].values)\n",
        "    frame_w, frame_h = tracks['size']\n",
        "    margin = cmd.params.get('margin', 0.10)\n",
        "    scale = np.maximum(widths / frame_w, heights / frame_h) * (1 + margin)\n",
        "    centers_x = smooth_series(centers_x, **SAVGOL_DEFAULT)\n",
        "    centers_y = smooth_series(centers_y, **SAVGOL_DEFAULT)\n",
        "    scale = smooth_series(scale, **SAVGOL_DEFAULT)\n",
        "    timeline = []\n",
        "    for idx, row in enumerate(window.itertuples(index=False)):\n",
        "        timeline.append({\n",
        "            't': float(row.t),\n",
        "            'frame': int(row.frame_index),\n",
        "            'center': [float(centers_x[idx]), float(centers_y[idx])],\n",
        "            'scale': float(scale[idx]),\n",
        "            'bbox': [float(row.x1), float(row.y1), float(row.x2), float(row.y2)],\n",
        "            'mask_rle': row.mask_rle\n",
        "        })\n",
        "    plan = {\n",
        "        'effect': cmd.effect,\n",
        "        'object': cmd.object,\n",
        "        'track_id': track_id,\n",
        "        't_in': cmd.t_in,\n",
        "        't_out': cmd.t_out,\n",
        "        'timeline': timeline,\n",
        "        'frame_size': tracks['size'],\n",
        "        'fps': tracks['fps'],\n",
        "        'video_path': tracks['video_path'],\n",
        "        'params': cmd.params\n",
        "    }\n",
        "    return plan\n",
        "\n",
        "\n",
        "def save_keyframes(plans: List[Dict[str, Any]]):\n",
        "    path = EXPORT_DIR / 'keyframes.json'\n",
        "    with open(path, 'w', encoding='utf-8') as f:\n",
        "        json.dump({'effects': plans}, f, indent=2)\n",
        "    print('Saved keyframes →', path)\n",
        "\n",
        "video_duration = tracks_data['duration']\n",
        "demo_commands = [\n",
        "    'zoom on the person from 1.0s to 4.5s with 12% margin',\n",
        "    'blur background around the person from 0s to 5s',\n",
        "    'callout the person from 2s to 6s label speaker'\n",
        "]\n",
        "parsed_cmds = [parse_nl_to_dsl(cmd, video_duration)[0] for cmd in demo_commands]\n",
        "effect_plans = [plan_effect(cmd, tracks_data) for cmd in parsed_cmds]\n",
        "save_keyframes(effect_plans)\n",
        "len(effect_plans)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b13551d",
      "metadata": {
        "id": "4b13551d"
      },
      "source": [
        "\n",
        "## Renderers (MoviePy + OpenCV)\n",
        "Implementations for all eight verbs; each writes `/content/exports/out_<effect>.mp4` and optionally previews inline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d818382",
      "metadata": {
        "id": "9d818382"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Rendering helpers\n",
        "import base64\n",
        "\n",
        "\n",
        "def decode_mask_rle(rle: Optional[Dict[str, Any]], frame_shape: Tuple[int, int]):\n",
        "    if not rle:\n",
        "        return None\n",
        "    mask = decode_mask(rle)\n",
        "    if mask.shape != frame_shape:\n",
        "        mask = cv2.resize(mask, (frame_shape[1], frame_shape[0]), interpolation=cv2.INTER_NEAREST)\n",
        "    return mask\n",
        "\n",
        "\n",
        "def timeline_sampler(timeline: List[Dict[str, Any]]):\n",
        "    times = np.array([item['t'] for item in timeline], dtype=np.float32)\n",
        "    centers = np.array([item['center'] for item in timeline], dtype=np.float32)\n",
        "    scales = np.array([item['scale'] for item in timeline], dtype=np.float32)\n",
        "    bboxes = np.array([item['bbox'] for item in timeline], dtype=np.float32)\n",
        "    masks = [item.get('mask_rle') for item in timeline]\n",
        "\n",
        "    def sample(t: float):\n",
        "        if t <= times[0]:\n",
        "            return {'center': centers[0], 'scale': scales[0], 'bbox': bboxes[0], 'mask': masks[0]}\n",
        "        if t >= times[-1]:\n",
        "            return {'center': centers[-1], 'scale': scales[-1], 'bbox': bboxes[-1], 'mask': masks[-1]}\n",
        "        idx = np.searchsorted(times, t, side='right')\n",
        "        i0 = max(idx - 1, 0)\n",
        "        i1 = min(idx, len(times) - 1)\n",
        "        span = (times[i1] - times[i0]) or 1e-6\n",
        "        alpha = (t - times[i0]) / span\n",
        "        center = centers[i0] * (1 - alpha) + centers[i1] * alpha\n",
        "        scale = scales[i0] * (1 - alpha) + scales[i1] * alpha\n",
        "        bbox = bboxes[i0] * (1 - alpha) + bboxes[i1] * alpha\n",
        "        mask = masks[i0 if alpha <= 0.5 else i1]\n",
        "        return {'center': center, 'scale': scale, 'bbox': bbox, 'mask': mask}\n",
        "\n",
        "    return sample\n",
        "\n",
        "\n",
        "def ensure_mask(state: Dict[str, Any], frame_shape: Tuple[int, int], feather: int = 25):\n",
        "    mask = decode_mask_rle(state.get('mask'), frame_shape)\n",
        "    if mask is None:\n",
        "        x1, y1, x2, y2 = state['bbox']\n",
        "        temp = np.zeros(frame_shape, dtype=np.uint8)\n",
        "        cv2.ellipse(\n",
        "            temp,\n",
        "            center=(int((x1 + x2) / 2), int((y1 + y2) / 2)),\n",
        "            axes=(int(max((x2 - x1) / 2, 1)), int(max((y2 - y1) / 2, 1))),\n",
        "            angle=0,\n",
        "            startAngle=0,\n",
        "            endAngle=360,\n",
        "            color=255,\n",
        "            thickness=-1\n",
        "        )\n",
        "        mask = temp\n",
        "    if feather > 0:\n",
        "        mask = cv2.GaussianBlur(mask, (0, 0), sigmaX=feather)\n",
        "    return np.clip(mask.astype(np.float32) / 255.0, 0, 1)[..., None]\n",
        "\n",
        "\n",
        "def clamp_window(center, scale, frame_size):\n",
        "    W, H = frame_size\n",
        "    crop_w = max(W * scale, 64)\n",
        "    crop_h = max(H * scale, 64)\n",
        "    x1 = np.clip(center[0] - crop_w / 2, 0, W - crop_w)\n",
        "    y1 = np.clip(center[1] - crop_h / 2, 0, H - crop_h)\n",
        "    x2 = x1 + crop_w\n",
        "    y2 = y1 + crop_h\n",
        "    return int(x1), int(y1), int(x2), int(y2)\n",
        "\n",
        "\n",
        "def run_moviepy(plan: Dict[str, Any], frame_fn, output_path: Path, codec: str = 'libx264'):\n",
        "    clip = VideoFileClip(plan['video_path'])\n",
        "    sampler = timeline_sampler(plan['timeline'])\n",
        "    H = int(plan['frame_size'][1])\n",
        "    W = int(plan['frame_size'][0])\n",
        "\n",
        "    def processor(get_frame, t):\n",
        "        frame = get_frame(t)\n",
        "        if t < plan['t_in'] or t > plan['t_out']:\n",
        "            return frame\n",
        "        state = sampler(t)\n",
        "        return frame_fn(frame, state, (H, W))\n",
        "\n",
        "    processed = clip.fl(processor)\n",
        "    processed.write_videofile(str(output_path), codec=codec, audio=True, audio_codec='aac', fps=clip.fps, logger=None)\n",
        "    clip.close()\n",
        "    processed.close()\n",
        "    return output_path\n",
        "\n",
        "\n",
        "def render_zoom_follow(plan: Dict[str, Any], output: Path):\n",
        "    W, H = plan['frame_size']\n",
        "\n",
        "    def fn(frame, state, shape):\n",
        "        cx, cy = state['center']\n",
        "        scale = np.clip(state['scale'], 0.2, 1.0)\n",
        "        x1, y1, x2, y2 = clamp_window((cx, cy), scale, (W, H))\n",
        "        cropped = frame[int(y1):int(y2), int(x1):int(x2)]\n",
        "        return cv2.resize(cropped, (W, H), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "    return run_moviepy(plan, fn, output)\n",
        "\n",
        "\n",
        "def render_spotlight(plan: Dict[str, Any], output: Path):\n",
        "    feather = int(plan['params'].get('feather', 45))\n",
        "    strength = plan['params'].get('strength', 0.7)\n",
        "\n",
        "    def fn(frame, state, shape):\n",
        "        mask = ensure_mask(state, shape, feather)\n",
        "        dimmed = (frame * (1 - strength)).astype(np.uint8)\n",
        "        return (frame * mask + dimmed * (1 - mask)).astype(np.uint8)\n",
        "\n",
        "    return run_moviepy(plan, fn, output)\n",
        "\n",
        "\n",
        "def render_blur_background(plan: Dict[str, Any], output: Path):\n",
        "    ksize = int(plan['params'].get('ksize', 21))\n",
        "    if ksize % 2 == 0:\n",
        "        ksize += 1\n",
        "\n",
        "    def fn(frame, state, shape):\n",
        "        mask = ensure_mask(state, shape, 25)\n",
        "        blurred = cv2.GaussianBlur(frame, (ksize, ksize), 0)\n",
        "        return (frame * mask + blurred * (1 - mask)).astype(np.uint8)\n",
        "\n",
        "    return run_moviepy(plan, fn, output)\n",
        "\n",
        "\n",
        "def render_pixelate(plan: Dict[str, Any], output: Path):\n",
        "    block = int(plan['params'].get('block', 20))\n",
        "\n",
        "    def fn(frame, state, shape):\n",
        "        mask = ensure_mask(state, shape, 5)\n",
        "        h, w, _ = frame.shape\n",
        "        small = cv2.resize(frame, (max(1, w // block), max(1, h // block)), interpolation=cv2.INTER_LINEAR)\n",
        "        pixelated = cv2.resize(small, (w, h), interpolation=cv2.INTER_NEAREST)\n",
        "        return (frame * (1 - mask) + pixelated * mask).astype(np.uint8)\n",
        "\n",
        "    return run_moviepy(plan, fn, output)\n",
        "\n",
        "\n",
        "def render_autoreframe(plan: Dict[str, Any], output: Path):\n",
        "    aspect = plan['params'].get('aspect', '9:16')\n",
        "    w_ratio, h_ratio = [int(x) for x in aspect.split(':')]\n",
        "    target_aspect = w_ratio / h_ratio\n",
        "    W, H = plan['frame_size']\n",
        "\n",
        "    def fn(frame, state, shape):\n",
        "        cx, cy = state['center']\n",
        "        cur_aspect = W / H\n",
        "        if cur_aspect > target_aspect:\n",
        "            crop_w = H * target_aspect\n",
        "            crop_h = H\n",
        "        else:\n",
        "            crop_w = W\n",
        "            crop_h = W / target_aspect\n",
        "        x1 = np.clip(cx - crop_w / 2, 0, W - crop_w)\n",
        "        y1 = np.clip(cy - crop_h / 2, 0, H - crop_h)\n",
        "        cropped = frame[int(y1):int(y1 + crop_h), int(x1):int(x1 + crop_w)]\n",
        "        target_h = 720\n",
        "        target_w = int(target_h * target_aspect)\n",
        "        return cv2.resize(cropped, (target_w, target_h))\n",
        "\n",
        "    return run_moviepy(plan, fn, output)\n",
        "\n",
        "\n",
        "def render_callout(plan: Dict[str, Any], output: Path):\n",
        "    label = plan['params'].get('label', plan['object'])\n",
        "\n",
        "    def fn(frame, state, shape):\n",
        "        frame_out = frame.copy()\n",
        "        cx, cy = map(int, state['center'])\n",
        "        anchor = (np.clip(cx + 100, 0, frame.shape[1] - 1), np.clip(cy - 100, 0, frame.shape[0] - 1))\n",
        "        cv2.line(frame_out, (cx, cy), anchor, (255, 255, 255), 2)\n",
        "        cv2.circle(frame_out, (cx, cy), 6, (0, 255, 0), -1)\n",
        "        box_w, box_h = 160, 60\n",
        "        x1 = np.clip(anchor[0], 0, frame.shape[1] - box_w - 1)\n",
        "        y1 = np.clip(anchor[1], 0, frame.shape[0] - box_h - 1)\n",
        "        x2, y2 = x1 + box_w, y1 + box_h\n",
        "        overlay = frame_out.copy()\n",
        "        cv2.rectangle(overlay, (x1, y1), (x2, y2), (0, 0, 0), -1)\n",
        "        cv2.addWeighted(overlay, 0.65, frame_out, 0.35, 0, frame_out)\n",
        "        cv2.rectangle(frame_out, (x1, y1), (x2, y2), (255, 255, 255), 2)\n",
        "        cv2.putText(frame_out, label, (x1 + 12, y1 + 35), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (255, 255, 255), 2)\n",
        "        return frame_out\n",
        "\n",
        "    return run_moviepy(plan, fn, output)\n",
        "\n",
        "\n",
        "def render_pip(plan: Dict[str, Any], output: Path):\n",
        "    scale = plan['params'].get('scale', 1.5)\n",
        "    radius = int(plan['params'].get('radius', 120))\n",
        "\n",
        "    def fn(frame, state, shape):\n",
        "        frame_out = frame.copy()\n",
        "        h, w, _ = frame_out.shape\n",
        "        cx, cy = map(int, state['center'])\n",
        "        x1 = max(cx - radius, 0)\n",
        "        y1 = max(cy - radius, 0)\n",
        "        x2 = min(cx + radius, w - 1)\n",
        "        y2 = min(cy + radius, h - 1)\n",
        "        patch = frame_out[y1:y2, x1:x2]\n",
        "        if patch.size == 0:\n",
        "            return frame_out\n",
        "        pip = cv2.resize(patch, (max(1, int(patch.shape[1] * scale)), max(1, int(patch.shape[0] * scale))))\n",
        "        pip_h, pip_w = pip.shape[:2]\n",
        "        target_x = int(np.clip(min(w - pip_w - 20, max(20, cx + radius)), 0, max(0, w - pip_w)))\n",
        "        target_y = int(np.clip(max(20, cy - radius - pip_h), 0, max(0, h - pip_h)))\n",
        "        frame_out[target_y:target_y + pip_h, target_x:target_x + pip_w] = pip\n",
        "        cv2.rectangle(frame_out, (target_x, target_y), (target_x + pip_w, target_y + pip_h), (255, 255, 255), 2)\n",
        "        return frame_out\n",
        "\n",
        "    return run_moviepy(plan, fn, output)\n",
        "\n",
        "\n",
        "def render_path(plan: Dict[str, Any], output: Path):\n",
        "    points = np.array([item['center'] for item in plan['timeline']], dtype=np.int32)\n",
        "\n",
        "    def fn(frame, state, shape):\n",
        "        frame_out = frame.copy()\n",
        "        cv2.polylines(frame_out, [points], False, (0, 255, 255), 4)\n",
        "        return frame_out\n",
        "\n",
        "    return run_moviepy(plan, fn, output)\n",
        "\n",
        "RENDERERS = {\n",
        "    'ZoomFollow': render_zoom_follow,\n",
        "    'Spotlight': render_spotlight,\n",
        "    'BlurBackground': render_blur_background,\n",
        "    'PixelateObject': render_pixelate,\n",
        "    'AutoReframe': render_autoreframe,\n",
        "    'Callout': render_callout,\n",
        "    'PiPMagnifier': render_pip,\n",
        "    'PathOverlay': render_path\n",
        "}\n",
        "\n",
        "\n",
        "def render_effect(plan: Dict[str, Any], name: Optional[str] = None, preview: bool = True) -> Path:\n",
        "    effect = name or plan['effect']\n",
        "    output = EXPORT_DIR / f'out_{effect.lower()}.mp4'\n",
        "    renderer = RENDERERS[effect]\n",
        "    renderer(plan, output)\n",
        "    if preview:\n",
        "        display(Video(str(output)))\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6bdbedd",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/content/exports/out_zoomfollow.mp4": {
              "data": "",
              "ok": false,
              "headers": [
                [
                  "content-length",
                  "0"
                ]
              ],
              "status": 404,
              "status_text": ""
            },
            "http://localhost:8080/content/exports/out_callout.mp4": {
              "data": "",
              "ok": false,
              "headers": [
                [
                  "content-length",
                  "0"
                ]
              ],
              "status": 404,
              "status_text": ""
            },
            "http://localhost:8080/content/exports/out_blurbackground.mp4": {
              "data": "",
              "ok": false,
              "headers": [
                [
                  "content-length",
                  "0"
                ]
              ],
              "status": 404,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "f6bdbedd",
        "outputId": "faf7b84e-e0b1-4317-9e4e-f6a06e253b13"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Video object>"
            ],
            "text/html": [
              "<video src=\"/content/exports/out_zoomfollow.mp4\" controls  >\n",
              "      Your browser does not support the <code>video</code> element.\n",
              "    </video>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Video object>"
            ],
            "text/html": [
              "<video src=\"/content/exports/out_blurbackground.mp4\" controls  >\n",
              "      Your browser does not support the <code>video</code> element.\n",
              "    </video>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Video object>"
            ],
            "text/html": [
              "<video src=\"/content/exports/out_callout.mp4\" controls  >\n",
              "      Your browser does not support the <code>video</code> element.\n",
              "    </video>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(PosixPath('/content/exports/out_zoomfollow.mp4'),\n",
              " PosixPath('/content/exports/out_blurbackground.mp4'),\n",
              " PosixPath('/content/exports/out_callout.mp4'))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "\n",
        "#@title Render demo effects (ZoomFollow, BlurBackground, Callout)\n",
        "zoom_plan, blur_plan, callout_plan = effect_plans\n",
        "zoom_path = render_effect(zoom_plan, 'ZoomFollow')\n",
        "blur_path = render_effect(blur_plan, 'BlurBackground')\n",
        "callout_path = render_effect(callout_plan, 'Callout')\n",
        "zoom_path, blur_path, callout_path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eec425db",
      "metadata": {
        "id": "eec425db"
      },
      "source": [
        "\n",
        "## Visual sanity checks & analytics\n",
        "Charts complement the video preview: class counts over time plus smoothed center/scale curves. Saved under `/content/exports/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61b4c61a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61b4c61a",
        "outputId": "7417eb07-d002-4b32-8d21-eaf1ca5ecaed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(PosixPath('/content/exports/plot_class_counts.png'),\n",
              " [PosixPath('/content/exports/plot_zoomfollow.png'),\n",
              "  PosixPath('/content/exports/plot_blurbackground.png'),\n",
              "  PosixPath('/content/exports/plot_callout.png')])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "\n",
        "#@title Generate plots\n",
        "class Plotter:\n",
        "    def __init__(self, tracks: Dict[str, Any], plans: List[Dict[str, Any]]):\n",
        "        self.tracks = tracks\n",
        "        self.df = get_tracks_df(tracks)\n",
        "        self.plans = plans\n",
        "\n",
        "    def class_counts(self) -> Path:\n",
        "        if self.df.empty:\n",
        "            raise ValueError('No detections to plot')\n",
        "        df = self.df.copy()\n",
        "        df['second'] = df['t'].round(1)\n",
        "        counts = df.groupby(['second', 'cls']).size().reset_index(name='detections')\n",
        "        fig, ax = plt.subplots(figsize=(8, 4))\n",
        "        for cls, group in counts.groupby('cls'):\n",
        "            ax.plot(group['second'], group['detections'], label=cls)\n",
        "        ax.set_xlabel('Time (s)')\n",
        "        ax.set_ylabel('Detections')\n",
        "        ax.set_title('Class counts over time')\n",
        "        ax.legend()\n",
        "        path = EXPORT_DIR / 'plot_class_counts.png'\n",
        "        fig.tight_layout()\n",
        "        fig.savefig(path)\n",
        "        plt.close(fig)\n",
        "        return path\n",
        "\n",
        "    def plan_curves(self, plan: Dict[str, Any]) -> Path:\n",
        "        times = [pt['t'] for pt in plan['timeline']]\n",
        "        centers_x = [pt['center'][0] for pt in plan['timeline']]\n",
        "        centers_y = [pt['center'][1] for pt in plan['timeline']]\n",
        "        scales = [pt['scale'] for pt in plan['timeline']]\n",
        "        fig, ax = plt.subplots(3, 1, figsize=(8, 8), sharex=True)\n",
        "        ax[0].plot(times, centers_x)\n",
        "        ax[0].set_ylabel('Center X')\n",
        "        ax[1].plot(times, centers_y, color='orange')\n",
        "        ax[1].set_ylabel('Center Y')\n",
        "        ax[2].plot(times, scales, color='green')\n",
        "        ax[2].set_ylabel('Scale %')\n",
        "        ax[2].set_xlabel('Time (s)')\n",
        "        fig.suptitle(f\"Plan curves · {plan['effect']} ({plan['object']})\")\n",
        "        path = EXPORT_DIR / f\"plot_{plan['effect'].lower()}.png\"\n",
        "        fig.tight_layout()\n",
        "        fig.savefig(path)\n",
        "        plt.close(fig)\n",
        "        return path\n",
        "\n",
        "plotter = Plotter(tracks_data, effect_plans)\n",
        "class_plot = plotter.class_counts()\n",
        "plan_plots = [plotter.plan_curves(plan) for plan in effect_plans]\n",
        "class_plot, plan_plots\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bf00a2b",
      "metadata": {
        "id": "8bf00a2b"
      },
      "source": [
        "\n",
        "## Simple chat-like UI\n",
        "ipywidgets controls for Analyze → Plan → Render plus preset prompts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c38c9042",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "c38c9042",
        "outputId": "e8949949-bffe-4d6e-9d67-6500dabde6ba"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tracks_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-657062806.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#@title Chat-style widgets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m state = {\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;34m'tracks'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtracks_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;34m'plans'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0meffect_plans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m }\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tracks_data' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "#@title Chat-style widgets\n",
        "state = {\n",
        "    'tracks': tracks_data,\n",
        "    'plans': effect_plans\n",
        "}\n",
        "\n",
        "video_dropdown = widgets.Dropdown(options=list(CLIP_PATHS.keys()), description='Clip', value='clipA')\n",
        "effect_dropdown = widgets.Dropdown(options=list(RENDERERS.keys()), description='Effect', value='ZoomFollow')\n",
        "prompt_box = widgets.Textarea(value=demo_commands[0], description='Command', layout=widgets.Layout(width='80%', height='80px'))\n",
        "output_area = widgets.Output()\n",
        "\n",
        "preset_buttons = [\n",
        "    widgets.Button(description='Zoom follow person'),\n",
        "    widgets.Button(description='Blur speaker'),\n",
        "    widgets.Button(description='Callout speaker')\n",
        "]\n",
        "\n",
        "preset_texts = demo_commands\n",
        "\n",
        "\n",
        "def on_preset_click(idx):\n",
        "    prompt_box.value = preset_texts[idx]\n",
        "\n",
        "for i, btn in enumerate(preset_buttons):\n",
        "    btn.on_click(lambda b, idx=i: on_preset_click(idx))\n",
        "\n",
        "analyze_btn = widgets.Button(description='Analyze', button_style='info')\n",
        "plan_btn = widgets.Button(description='Plan', button_style='warning')\n",
        "render_btn = widgets.Button(description='Render', button_style='success')\n",
        "\n",
        "\n",
        "def handle_analyze(_):\n",
        "    output_area.clear_output()\n",
        "    with output_area:\n",
        "        clip_key = video_dropdown.value\n",
        "        state['tracks'] = detect_and_track(str(CLIP_PATHS[clip_key]), use_seg=True, save_json=True)\n",
        "        print('Recomputed tracks for', clip_key)\n",
        "\n",
        "\n",
        "def handle_plan(_):\n",
        "    output_area.clear_output()\n",
        "    with output_area:\n",
        "        cmds = parse_nl_to_dsl(prompt_box.value, state['tracks']['duration'])\n",
        "        plans = [plan_effect(cmd, state['tracks']) for cmd in cmds]\n",
        "        state['plans'] = plans\n",
        "        save_keyframes(plans)\n",
        "        print(f'Planned {len(plans)} effect(s)')\n",
        "\n",
        "\n",
        "def handle_render(_):\n",
        "    output_area.clear_output()\n",
        "    with output_area:\n",
        "        plan = next((p for p in state['plans'] if p['effect'] == effect_dropdown.value), state['plans'][0])\n",
        "        path = render_effect(plan, effect_dropdown.value)\n",
        "        print('Rendered →', path)\n",
        "\n",
        "analyze_btn.on_click(handle_analyze)\n",
        "plan_btn.on_click(handle_plan)\n",
        "render_btn.on_click(handle_render)\n",
        "\n",
        "ui = widgets.VBox([\n",
        "    widgets.HBox([video_dropdown, effect_dropdown]),\n",
        "    prompt_box,\n",
        "    widgets.HBox(preset_buttons),\n",
        "    widgets.HBox([analyze_btn, plan_btn, render_btn]),\n",
        "    output_area\n",
        "])\n",
        "display(ui)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41238aa4",
      "metadata": {
        "id": "41238aa4"
      },
      "source": [
        "\n",
        "## Optional FastAPI shim\n",
        "A thin service layer showing how to reuse the notebook helpers over HTTP (Analyze → Plan → Render).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ef68250",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ef68250",
        "outputId": "46b5dd58-319c-4c28-febc-50cc0fb786ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastAPI app ready → launch with: uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#@title FastAPI stub (service contract)\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "\n",
        "app = FastAPI(title='Object Tracking Effects API')\n",
        "\n",
        "class AnalyzeRequest(BaseModel):\n",
        "    video_path: str\n",
        "    classes: Optional[List[str]] = None\n",
        "    tracker: str = 'bytetrack'\n",
        "    use_seg: bool = False\n",
        "\n",
        "class PlanRequest(BaseModel):\n",
        "    command: str\n",
        "    video_duration: float\n",
        "\n",
        "class RenderRequest(BaseModel):\n",
        "    plan: Dict[str, Any]\n",
        "    effect: Optional[str] = None\n",
        "\n",
        "@app.post('/analyze')\n",
        "def analyze(req: AnalyzeRequest):\n",
        "    data = detect_and_track(req.video_path, req.classes, req.tracker, use_seg=req.use_seg)\n",
        "    return data\n",
        "\n",
        "@app.post('/plan')\n",
        "def plan(req: PlanRequest):\n",
        "    cmds = parse_nl_to_dsl(req.command, req.video_duration)\n",
        "    plans = [plan_effect(cmd, tracks_data) for cmd in cmds]\n",
        "    save_keyframes(plans)\n",
        "    return {'plans': plans}\n",
        "\n",
        "@app.post('/render')\n",
        "def render(req: RenderRequest):\n",
        "    path = render_effect(req.plan, req.effect, preview=False)\n",
        "    return {'output': str(path)}\n",
        "\n",
        "print('FastAPI app ready → launch with: uvicorn.run(app, host=\"0.0.0.0\", port=8000)')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}